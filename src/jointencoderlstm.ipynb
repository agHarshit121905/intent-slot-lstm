{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e221e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 2.2237\n",
      "Intent Acc: 0.9579 | Slot P: 0.3011 R: 0.3011 F1: 0.3011\n",
      "Epoch 2 | Loss: 0.6364\n",
      "Intent Acc: 0.9823 | Slot P: 0.3308 R: 0.3308 F1: 0.3308\n",
      "Epoch 3 | Loss: 0.3131\n",
      "Intent Acc: 0.9845 | Slot P: 0.3432 R: 0.3432 F1: 0.3432\n",
      "Epoch 4 | Loss: 0.1896\n",
      "Intent Acc: 0.9911 | Slot P: 0.3494 R: 0.3494 F1: 0.3494\n",
      "Epoch 5 | Loss: 0.1183\n",
      "Intent Acc: 0.9911 | Slot P: 0.3526 R: 0.3526 F1: 0.3526\n",
      "Epoch 6 | Loss: 0.0756\n",
      "Intent Acc: 0.9867 | Slot P: 0.3543 R: 0.3543 F1: 0.3543\n",
      "Epoch 7 | Loss: 0.0527\n",
      "Intent Acc: 0.9933 | Slot P: 0.3547 R: 0.3547 F1: 0.3547\n",
      "Epoch 8 | Loss: 0.0370\n",
      "Intent Acc: 0.9933 | Slot P: 0.3547 R: 0.3547 F1: 0.3547\n",
      "Epoch 9 | Loss: 0.0262\n",
      "Intent Acc: 0.9933 | Slot P: 0.3549 R: 0.3549 F1: 0.3549\n",
      "Epoch 10 | Loss: 0.0202\n",
      "Intent Acc: 0.9933 | Slot P: 0.3549 R: 0.3549 F1: 0.3549\n",
      "Epoch 11 | Loss: 0.0166\n",
      "Intent Acc: 0.9933 | Slot P: 0.3551 R: 0.3551 F1: 0.3551\n",
      "Epoch 12 | Loss: 0.0131\n",
      "Intent Acc: 0.9933 | Slot P: 0.3543 R: 0.3543 F1: 0.3543\n",
      "Epoch 13 | Loss: 0.0126\n",
      "Intent Acc: 0.9933 | Slot P: 0.3554 R: 0.3554 F1: 0.3554\n",
      "Epoch 14 | Loss: 0.0085\n",
      "Intent Acc: 0.9933 | Slot P: 0.3551 R: 0.3551 F1: 0.3551\n",
      "Epoch 15 | Loss: 0.0068\n",
      "Intent Acc: 0.9933 | Slot P: 0.3553 R: 0.3553 F1: 0.3553\n",
      "Epoch 16 | Loss: 0.0058\n",
      "Intent Acc: 0.9933 | Slot P: 0.3553 R: 0.3553 F1: 0.3553\n",
      "Epoch 17 | Loss: 0.0049\n",
      "Intent Acc: 0.9933 | Slot P: 0.3558 R: 0.3558 F1: 0.3558\n",
      "Epoch 18 | Loss: 0.0041\n",
      "Intent Acc: 0.9933 | Slot P: 0.3556 R: 0.3556 F1: 0.3556\n",
      "Epoch 19 | Loss: 0.0038\n",
      "Intent Acc: 0.9933 | Slot P: 0.3558 R: 0.3558 F1: 0.3558\n",
      "Epoch 20 | Loss: 0.0037\n",
      "Intent Acc: 0.9933 | Slot P: 0.3558 R: 0.3558 F1: 0.3558\n",
      "Epoch 21 | Loss: 0.0030\n",
      "Intent Acc: 0.9933 | Slot P: 0.3558 R: 0.3558 F1: 0.3558\n",
      "Epoch 22 | Loss: 0.0027\n",
      "Intent Acc: 0.9933 | Slot P: 0.3560 R: 0.3560 F1: 0.3560\n",
      "Epoch 23 | Loss: 0.0027\n",
      "Intent Acc: 0.9933 | Slot P: 0.3556 R: 0.3556 F1: 0.3556\n",
      "Epoch 24 | Loss: 0.0022\n",
      "Intent Acc: 0.9933 | Slot P: 0.3554 R: 0.3554 F1: 0.3554\n",
      "Epoch 25 | Loss: 0.0019\n",
      "Intent Acc: 0.9933 | Slot P: 0.3558 R: 0.3558 F1: 0.3558\n",
      "Epoch 26 | Loss: 0.0018\n",
      "Intent Acc: 0.9933 | Slot P: 0.3556 R: 0.3556 F1: 0.3556\n",
      "Epoch 27 | Loss: 0.0015\n",
      "Intent Acc: 0.9933 | Slot P: 0.3560 R: 0.3560 F1: 0.3560\n",
      "Epoch 28 | Loss: 0.0014\n",
      "Intent Acc: 0.9933 | Slot P: 0.3558 R: 0.3558 F1: 0.3558\n",
      "Epoch 29 | Loss: 0.0012\n",
      "Intent Acc: 0.9933 | Slot P: 0.3558 R: 0.3558 F1: 0.3558\n",
      "Epoch 30 | Loss: 0.0013\n",
      "Intent Acc: 0.9933 | Slot P: 0.3558 R: 0.3558 F1: 0.3558\n",
      "Epoch 31 | Loss: 0.0011\n",
      "Intent Acc: 0.9933 | Slot P: 0.3558 R: 0.3558 F1: 0.3558\n",
      "Epoch 32 | Loss: 0.0012\n",
      "Intent Acc: 0.9933 | Slot P: 0.3566 R: 0.3566 F1: 0.3566\n",
      "Epoch 33 | Loss: 0.0015\n",
      "Intent Acc: 0.9933 | Slot P: 0.3562 R: 0.3562 F1: 0.3562\n",
      "Epoch 34 | Loss: 0.0013\n",
      "Intent Acc: 0.9933 | Slot P: 0.3556 R: 0.3556 F1: 0.3556\n",
      "Epoch 35 | Loss: 0.0299\n",
      "Intent Acc: 0.9933 | Slot P: 0.3564 R: 0.3564 F1: 0.3564\n",
      "Epoch 36 | Loss: 0.0274\n",
      "Intent Acc: 0.9867 | Slot P: 0.3560 R: 0.3560 F1: 0.3560\n",
      "Epoch 37 | Loss: 0.0077\n",
      "Intent Acc: 0.9956 | Slot P: 0.3573 R: 0.3573 F1: 0.3573\n",
      "Epoch 38 | Loss: 0.0045\n",
      "Intent Acc: 0.9956 | Slot P: 0.3579 R: 0.3579 F1: 0.3579\n",
      "Epoch 39 | Loss: 0.0022\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 40 | Loss: 0.0015\n",
      "Intent Acc: 0.9956 | Slot P: 0.3569 R: 0.3569 F1: 0.3569\n",
      "Epoch 41 | Loss: 0.0013\n",
      "Intent Acc: 0.9956 | Slot P: 0.3573 R: 0.3573 F1: 0.3573\n",
      "Epoch 42 | Loss: 0.0013\n",
      "Intent Acc: 0.9956 | Slot P: 0.3573 R: 0.3573 F1: 0.3573\n",
      "Epoch 43 | Loss: 0.0011\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 44 | Loss: 0.0009\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 45 | Loss: 0.0010\n",
      "Intent Acc: 0.9956 | Slot P: 0.3577 R: 0.3577 F1: 0.3577\n",
      "Epoch 46 | Loss: 0.0007\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 47 | Loss: 0.0010\n",
      "Intent Acc: 0.9956 | Slot P: 0.3577 R: 0.3577 F1: 0.3577\n",
      "Epoch 48 | Loss: 0.0007\n",
      "Intent Acc: 0.9956 | Slot P: 0.3573 R: 0.3573 F1: 0.3573\n",
      "Epoch 49 | Loss: 0.0006\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 50 | Loss: 0.0005\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 51 | Loss: 0.0006\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 52 | Loss: 0.0006\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 53 | Loss: 0.0004\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 54 | Loss: 0.0005\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 55 | Loss: 0.0004\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 56 | Loss: 0.0005\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 57 | Loss: 0.0006\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 58 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3577 R: 0.3577 F1: 0.3577\n",
      "Epoch 59 | Loss: 0.0004\n",
      "Intent Acc: 0.9956 | Slot P: 0.3573 R: 0.3573 F1: 0.3573\n",
      "Epoch 60 | Loss: 0.0004\n",
      "Intent Acc: 0.9956 | Slot P: 0.3573 R: 0.3573 F1: 0.3573\n",
      "Epoch 61 | Loss: 0.0005\n",
      "Intent Acc: 0.9956 | Slot P: 0.3573 R: 0.3573 F1: 0.3573\n",
      "Epoch 62 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 63 | Loss: 0.0002\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 64 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3577 R: 0.3577 F1: 0.3577\n",
      "Epoch 65 | Loss: 0.0004\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 66 | Loss: 0.0006\n",
      "Intent Acc: 0.9956 | Slot P: 0.3577 R: 0.3577 F1: 0.3577\n",
      "Epoch 67 | Loss: 0.0002\n",
      "Intent Acc: 0.9956 | Slot P: 0.3577 R: 0.3577 F1: 0.3577\n",
      "Epoch 68 | Loss: 0.0004\n",
      "Intent Acc: 0.9956 | Slot P: 0.3577 R: 0.3577 F1: 0.3577\n",
      "Epoch 69 | Loss: 0.0002\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 70 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 71 | Loss: 0.0005\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 72 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 73 | Loss: 0.0002\n",
      "Intent Acc: 0.9956 | Slot P: 0.3573 R: 0.3573 F1: 0.3573\n",
      "Epoch 74 | Loss: 0.0002\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 75 | Loss: 0.0002\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 76 | Loss: 0.0004\n",
      "Intent Acc: 0.9956 | Slot P: 0.3579 R: 0.3579 F1: 0.3579\n",
      "Epoch 77 | Loss: 0.0004\n",
      "Intent Acc: 0.9956 | Slot P: 0.3579 R: 0.3579 F1: 0.3579\n",
      "Epoch 78 | Loss: 0.0001\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 79 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3579 R: 0.3579 F1: 0.3579\n",
      "Epoch 80 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3577 R: 0.3577 F1: 0.3577\n",
      "Epoch 81 | Loss: 0.0002\n",
      "Intent Acc: 0.9956 | Slot P: 0.3577 R: 0.3577 F1: 0.3577\n",
      "Epoch 82 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3577 R: 0.3577 F1: 0.3577\n",
      "Epoch 83 | Loss: 0.0002\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 84 | Loss: 0.0004\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 85 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3573 R: 0.3573 F1: 0.3573\n",
      "Epoch 86 | Loss: 0.0002\n",
      "Intent Acc: 0.9956 | Slot P: 0.3573 R: 0.3573 F1: 0.3573\n",
      "Epoch 87 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3575 R: 0.3575 F1: 0.3575\n",
      "Epoch 88 | Loss: 0.0093\n",
      "Intent Acc: 0.9956 | Slot P: 0.3556 R: 0.3556 F1: 0.3556\n",
      "Epoch 89 | Loss: 0.0196\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 90 | Loss: 0.0113\n",
      "Intent Acc: 0.9956 | Slot P: 0.3568 R: 0.3568 F1: 0.3568\n",
      "Epoch 91 | Loss: 0.0028\n",
      "Intent Acc: 0.9956 | Slot P: 0.3569 R: 0.3569 F1: 0.3569\n",
      "Epoch 92 | Loss: 0.0008\n",
      "Intent Acc: 0.9956 | Slot P: 0.3569 R: 0.3569 F1: 0.3569\n",
      "Epoch 93 | Loss: 0.0005\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 94 | Loss: 0.0006\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 95 | Loss: 0.0006\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 96 | Loss: 0.0004\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 97 | Loss: 0.0007\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 98 | Loss: 0.0007\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 99 | Loss: 0.0002\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "Epoch 100 | Loss: 0.0003\n",
      "Intent Acc: 0.9956 | Slot P: 0.3571 R: 0.3571 F1: 0.3571\n",
      "=== Test Set Evaluation ===\n",
      "Intent Acc: 0.9894 | Slot P: 0.4002 R: 0.4002 F1: 0.4002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9893899204244032,\n",
       " 0.40022463496817673,\n",
       " 0.40022463496817673,\n",
       " 0.40022463496817673)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "all_tokens = []\n",
    "for sentence in train_df['text']:\n",
    "    all_tokens.extend(sentence.split())\n",
    "vocab = sorted(set(all_tokens))\n",
    "word2idx = {\"<pad>\":0, \"<unk>\":1}\n",
    "for i, w in enumerate(vocab, start=2):\n",
    "    word2idx[w] = i\n",
    "\n",
    "with open(\"word2idx.json\", \"w\") as f:\n",
    "    json.dump(word2idx, f)\n",
    "\n",
    "\n",
    "all_slots = set()\n",
    "for slots in train_df['slots']:\n",
    "    all_slots.update(slots.split())\n",
    "slot2id = {\"O\":0}\n",
    "i = 1\n",
    "for s in all_slots:\n",
    "    if s != \"O\":\n",
    "        slot2id[s] = i\n",
    "        i +=1\n",
    "with open(\"slot2id.json\", \"w\") as f:\n",
    "    json.dump(slot2id, f)\n",
    "\n",
    "all_intents = train_df['intent'].unique()\n",
    "intent2id = {intent:i for i,intent in enumerate(all_intents)}\n",
    "with open(\"intent2id.json\", \"w\") as f:\n",
    "    json.dump(intent2id, f)\n",
    "\n",
    "\n",
    "id2slot = {v:k for k,v in slot2id.items()}\n",
    "id2intent = {v:k for k,v in intent2id.items()}\n",
    "\n",
    "class ATISDataset(Dataset):\n",
    "    def __init__(self, df, word2idx, slot2id, intent2id, max_len=50):\n",
    "        self.df = df\n",
    "        self.word2idx = word2idx\n",
    "        self.slot2id = slot2id\n",
    "        self.intent2id = intent2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.df.iloc[idx]['text'].split()\n",
    "        slots = self.df.iloc[idx]['slots'].split()\n",
    "        intent = self.df.iloc[idx]['intent']\n",
    "\n",
    "        length = len(tokens)\n",
    "        input_ids = [self.word2idx.get(t, self.word2idx[\"<unk>\"]) for t in tokens]\n",
    "        slot_ids = [self.slot2id.get(s, 0) for s in slots]  # default O=0\n",
    "        intent_id = self.intent2id[intent]\n",
    "\n",
    "        pad_len = self.max_len - length\n",
    "        input_ids += [self.word2idx[\"<pad>\"]] * pad_len\n",
    "        slot_ids += [0] * pad_len  \n",
    "\n",
    "        return torch.tensor(input_ids), torch.tensor(length), torch.tensor(slot_ids), torch.tensor(intent_id)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    lengths = torch.tensor([item[1] for item in batch])\n",
    "    slot_labels = torch.stack([item[2] for item in batch])\n",
    "    intent_labels = torch.stack([item[3] for item in batch])\n",
    "    return input_ids, lengths, slot_labels, intent_labels\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = ATISDataset(train_df, word2idx, slot2id, intent2id)\n",
    "val_dataset = ATISDataset(val_df, word2idx, slot2id, intent2id)\n",
    "test_dataset = ATISDataset(test_df, word2idx, slot2id, intent2id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "class JointLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, slot_label_size, intent_label_size, dropout=0.3):\n",
    "        super(JointLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.slot_classifier = nn.Linear(hidden_dim*2, slot_label_size)\n",
    "        self.intent_classifier = nn.Linear(hidden_dim*2, intent_label_size)\n",
    "\n",
    "    def forward(self, input_ids, lengths):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (hidden, cell) = self.encoder(packed)\n",
    "        sequence_output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True, total_length=input_ids.size(1))\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        intent_logits = self.intent_classifier(hidden_cat)\n",
    "        return slot_logits, intent_logits\n",
    "\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "slot_label_size = len(slot2id)\n",
    "intent_label_size = len(intent2id)\n",
    "\n",
    "model = JointLSTMModel(vocab_size, embedding_dim, hidden_dim, slot_label_size, intent_label_size).to(device)\n",
    "slot_loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "intent_loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, epochs=10, lambda_intent=0.5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for input_ids, lengths, slot_labels, intent_labels in train_loader:\n",
    "            input_ids, lengths = input_ids.to(device), lengths.to(device)\n",
    "            slot_labels, intent_labels = slot_labels.to(device), intent_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            slot_logits, intent_logits = model(input_ids, lengths)\n",
    "\n",
    "            slot_loss = slot_loss_fn(slot_logits.view(-1, slot_logits.shape[-1]), slot_labels.view(-1))\n",
    "            intent_loss = intent_loss_fn(intent_logits, intent_labels)\n",
    "            loss = slot_loss + lambda_intent * intent_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        evaluate(model, val_loader)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_intent_preds, all_intent_labels = [], []\n",
    "    all_slot_preds, all_slot_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, lengths, slot_labels, intent_labels in loader:\n",
    "            input_ids, lengths = input_ids.to(device), lengths.to(device)\n",
    "            slot_labels, intent_labels = slot_labels.to(device), intent_labels.to(device)\n",
    "\n",
    "            slot_logits, intent_logits = model(input_ids, lengths)\n",
    "            intent_preds = torch.argmax(intent_logits, dim=1)\n",
    "            all_intent_preds.extend(intent_preds.cpu().tolist())\n",
    "            all_intent_labels.extend(intent_labels.cpu().tolist())\n",
    "\n",
    "            slot_preds = torch.argmax(slot_logits, dim=2)\n",
    "            for i, l in enumerate(lengths):\n",
    "                all_slot_preds.extend(slot_preds[i][:l].cpu().tolist())\n",
    "                all_slot_labels.extend(slot_labels[i][:l].cpu().tolist())\n",
    "\n",
    "    intent_acc = accuracy_score(all_intent_labels, all_intent_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_slot_labels, all_slot_preds, average='micro')\n",
    "    print(f\"Intent Acc: {intent_acc:.4f} | Slot P: {precision:.4f} R: {recall:.4f} F1: {f1:.4f}\")\n",
    "    model.train()\n",
    "    return intent_acc, precision, recall, f1\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer, epochs=100)\n",
    "\n",
    "\n",
    "print(\"=== Test Set Evaluation ===\")\n",
    "evaluate(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
