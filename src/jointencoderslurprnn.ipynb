{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f98773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 6.0000\n",
      "=== Validation ===\n",
      "Intent Acc: 0.3002 | Slot P: 0.0506 R: 0.0506 F1: 0.0506\n",
      "Epoch 2 | Train Loss: 4.1937\n",
      "=== Validation ===\n",
      "Intent Acc: 0.5863 | Slot P: 0.0859 R: 0.0859 F1: 0.0859\n",
      "Epoch 3 | Train Loss: 2.8905\n",
      "=== Validation ===\n",
      "Intent Acc: 0.7293 | Slot P: 0.1115 R: 0.1115 F1: 0.1115\n",
      "Epoch 4 | Train Loss: 2.0693\n",
      "=== Validation ===\n",
      "Intent Acc: 0.7797 | Slot P: 0.1293 R: 0.1293 F1: 0.1293\n",
      "Warning: NaN or Inf loss detected at epoch 5, skipping batch\n",
      "Epoch 5 | Train Loss: 1.5759\n",
      "=== Validation ===\n",
      "Intent Acc: 0.7884 | Slot P: 0.1419 R: 0.1419 F1: 0.1419\n",
      "Epoch 6 | Train Loss: 1.2451\n",
      "=== Validation ===\n",
      "Intent Acc: 0.8086 | Slot P: 0.1517 R: 0.1517 F1: 0.1517\n",
      "Epoch 7 | Train Loss: 1.0401\n",
      "=== Validation ===\n",
      "Intent Acc: 0.7992 | Slot P: 0.1551 R: 0.1551 F1: 0.1551\n",
      "Warning: NaN or Inf loss detected at epoch 8, skipping batch\n",
      "Epoch 8 | Train Loss: 0.8555\n",
      "=== Validation ===\n",
      "Intent Acc: 0.8120 | Slot P: 0.1597 R: 0.1597 F1: 0.1597\n",
      "=== Test Set Evaluation ===\n",
      "Intent Acc: 0.8031 | Slot P: 0.1562 R: 0.1562 F1: 0.1562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.803065490013934,\n",
       " 0.15618755827893965,\n",
       " 0.15618755827893965,\n",
       " 0.15618755827893965)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def jsonl_to_df(jsonl_file):\n",
    "    texts = []\n",
    "    slot_seqs = []\n",
    "    intents = []\n",
    "\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            sentence = data[\"sentence\"]\n",
    "            tokens = sentence.split()\n",
    "            slots = [\"O\"] * len(tokens)\n",
    "\n",
    "            for entity in data.get(\"entities\", []):\n",
    "                span = entity[\"span\"]\n",
    "                typ = entity[\"type\"]\n",
    "                for i, token_idx in enumerate(span):\n",
    "                    if token_idx < len(tokens):\n",
    "                        if i == 0:\n",
    "                            slots[token_idx] = f\"B-{typ}\"\n",
    "                        else:\n",
    "                            slots[token_idx] = f\"I-{typ}\"\n",
    "\n",
    "            texts.append(sentence)\n",
    "            slot_seqs.append(\" \".join(slots))\n",
    "            intents.append(data[\"intent\"])\n",
    "\n",
    "    df = pd.DataFrame({\"text\": texts, \"slots\": slot_seqs, \"intent\": intents})\n",
    "    return df\n",
    "\n",
    "train_df = jsonl_to_df(\"train.jsonl\")\n",
    "val_df = jsonl_to_df(\"devel.jsonl\")\n",
    "test_df = jsonl_to_df(\"test.jsonl\")\n",
    "\n",
    "\n",
    "all_tokens = []\n",
    "for sentence in train_df['text']:\n",
    "    all_tokens.extend(sentence.split())\n",
    "vocab = sorted(set(all_tokens))\n",
    "word2idx = {\"<pad>\":0, \"<unk>\":1}\n",
    "for i, w in enumerate(vocab, start=2):\n",
    "    word2idx[w] = i\n",
    "\n",
    "all_slots = set()\n",
    "for slots in train_df['slots']:\n",
    "    all_slots.update(slots.split())\n",
    "slot2id = {\"O\":0}\n",
    "i = 1\n",
    "for s in all_slots:\n",
    "    if s != \"O\":\n",
    "        slot2id[s] = i\n",
    "        i +=1\n",
    "\n",
    "all_intents = train_df['intent'].unique()\n",
    "intent2id = {intent:i for i,intent in enumerate(all_intents)}\n",
    "\n",
    "id2slot = {v:k for k,v in slot2id.items()}\n",
    "id2intent = {v:k for k,v in intent2id.items()}\n",
    "\n",
    "\n",
    "class SLURPDataset(Dataset):\n",
    "    def __init__(self, df, word2idx, slot2id, intent2id, max_len=50):\n",
    "        self.df = df\n",
    "        self.word2idx = word2idx\n",
    "        self.slot2id = slot2id\n",
    "        self.intent2id = intent2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.df.iloc[idx]['text'].split()\n",
    "        slots = self.df.iloc[idx]['slots'].split()\n",
    "        intent = self.df.iloc[idx]['intent']\n",
    "\n",
    "        length = len(tokens)\n",
    "        input_ids = [self.word2idx.get(t, self.word2idx[\"<unk>\"]) for t in tokens]\n",
    "        slot_ids = [self.slot2id.get(s, 0) for s in slots]\n",
    "        intent_id = self.intent2id[intent]\n",
    "\n",
    "        pad_len = self.max_len - length\n",
    "        input_ids += [self.word2idx[\"<pad>\"]] * pad_len\n",
    "        slot_ids += [0] * pad_len\n",
    "\n",
    "        return torch.tensor(input_ids), torch.tensor(length), torch.tensor(slot_ids), torch.tensor(intent_id)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    lengths = torch.stack([item[1] for item in batch])\n",
    "    slot_labels = torch.stack([item[2] for item in batch])\n",
    "    intent_labels = torch.stack([item[3] for item in batch])\n",
    "    return input_ids, lengths, slot_labels, intent_labels\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = SLURPDataset(train_df, word2idx, slot2id, intent2id)\n",
    "val_dataset = SLURPDataset(val_df, word2idx, slot2id, intent2id)\n",
    "test_dataset = SLURPDataset(test_df, word2idx, slot2id, intent2id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "class JointRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, slot_label_size, intent_label_size, dropout=0.3):\n",
    "        super(JointRNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.slot_classifier = nn.Linear(hidden_dim*2, slot_label_size)\n",
    "        self.intent_classifier = nn.Linear(hidden_dim*2, intent_label_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, input_ids, lengths):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (hidden, cell) = self.encoder(packed)\n",
    "        sequence_output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True, total_length=input_ids.size(1))\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        intent_logits = self.intent_classifier(hidden_cat)\n",
    "        return slot_logits, intent_logits\n",
    "\n",
    "\n",
    "vocab_size = max(word2idx.values()) + 1\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "slot_label_size = len(slot2id)\n",
    "intent_label_size = len(intent2id)\n",
    "\n",
    "model = JointRNNModel(vocab_size, embedding_dim, hidden_dim, slot_label_size, intent_label_size, dropout=0.5).to(device)\n",
    "slot_loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "intent_loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)  # Added weight decay for regularization\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_intent_preds, all_intent_labels = [], []\n",
    "    all_slot_preds, all_slot_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, lengths, slot_labels, intent_labels in loader:\n",
    "            input_ids, lengths = input_ids.to(device), lengths.to(device)\n",
    "            slot_labels, intent_labels = slot_labels.to(device), intent_labels.to(device)\n",
    "\n",
    "            slot_logits, intent_logits = model(input_ids, lengths)\n",
    "            intent_preds = torch.argmax(intent_logits, dim=1)\n",
    "            all_intent_preds.extend(intent_preds.cpu().tolist())\n",
    "            all_intent_labels.extend(intent_labels.cpu().tolist())\n",
    "\n",
    "            slot_preds = torch.argmax(slot_logits, dim=2)\n",
    "            for i, l in enumerate(lengths):\n",
    "                all_slot_preds.extend(slot_preds[i][:l].cpu().tolist())\n",
    "                all_slot_labels.extend(slot_labels[i][:l].cpu().tolist())\n",
    "\n",
    "    intent_acc = accuracy_score(all_intent_labels, all_intent_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_slot_labels, all_slot_preds, average='micro')\n",
    "    print(f\"Intent Acc: {intent_acc:.4f} | Slot P: {precision:.4f} R: {recall:.4f} F1: {f1:.4f}\")\n",
    "    model.train()\n",
    "    return intent_acc, precision, recall, f1\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, epochs=10, lambda_intent=0.5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for input_ids, lengths, slot_labels, intent_labels in train_loader:\n",
    "            input_ids, lengths = input_ids.to(device), lengths.to(device)\n",
    "            slot_labels, intent_labels = slot_labels.to(device), intent_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            slot_logits, intent_logits = model(input_ids, lengths)\n",
    "\n",
    "            slot_loss = slot_loss_fn(slot_logits.view(-1, slot_logits.shape[-1]), slot_labels.view(-1))\n",
    "            intent_loss = intent_loss_fn(intent_logits, intent_labels)\n",
    "            loss = slot_loss + lambda_intent * intent_loss\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Warning: NaN or Inf loss detected at epoch {epoch+1}, skipping batch\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        print(\"=== Validation ===\")\n",
    "        evaluate(model, val_loader)\n",
    "\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer, epochs=8, lambda_intent=1.0)\n",
    "\n",
    "print(\"=== Test Set Evaluation ===\")\n",
    "evaluate(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
