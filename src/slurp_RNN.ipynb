{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f886207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  No file paths provided. Please call main() with file paths:\n",
      "Example: main('train.jsonl', 'val.jsonl', 'test.jsonl', 'slurp')\n",
      "Running in Jupyter Notebook mode\n",
      "Dataset type: slurp\n",
      "================================================================================\n",
      "Loading ../cleaned-datasets/slurp/train.jsonl...\n",
      "\n",
      "  First sample structure: ['sentence', 'sentence_annotation', 'intent', 'action', 'scenario', 'entities']\n",
      "  Sample preview: {'sentence': 'event', 'sentence_annotation': 'event', 'intent': 'calendar_set', 'action': 'set', 'scenario': 'calendar', 'entities': []}...\n",
      "Loaded 8129 valid samples\n",
      "Loading ../cleaned-datasets/slurp/devel.jsonl...\n",
      "\n",
      "  First sample structure: ['sentence', 'sentence_annotation', 'intent', 'action', 'scenario', 'entities']\n",
      "  Sample preview: {'sentence': 'siri what is one american dollar in japanese yen', 'sentence_annotation': 'siri what is one [currency_name : american dollar] in [currency_name : japanese yen]', 'intent': 'qa_currency',...\n",
      "Loaded 1489 valid samples\n",
      "Loading ../cleaned-datasets/slurp/test.jsonl...\n",
      "\n",
      "  First sample structure: ['sentence', 'sentence_annotation', 'intent', 'action', 'scenario', 'entities']\n",
      "  Sample preview: {'sentence': 'event reminder mona tuesday', 'sentence_annotation': 'event reminder [event_name : mona] [date : tuesday]', 'intent': 'calendar_set', 'action': 'set', 'scenario': 'calendar', 'entities':...\n",
      "Loaded 2153 valid samples\n",
      "\n",
      "Building vocabulary and mappings...\n",
      "Vocabulary size: 5041\n",
      "Number of intents: 24\n",
      "Number of slots: 45\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: Training Intent Classifier (RNN)\n",
      "================================================================================\n",
      "Training Intent Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|████████████████████████| 254/254 [00:09<00:00, 27.47it/s, loss=3.0695, acc=0.0728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Loss: 3.1136, Train Acc: 0.0728, Val Acc: 0.0734\n",
      "  ✓ New best validation accuracy: 0.0734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|████████████████████████| 254/254 [00:10<00:00, 24.73it/s, loss=3.0956, acc=0.0828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Loss: 3.0851, Train Acc: 0.0828, Val Acc: 0.0815\n",
      "  ✓ New best validation accuracy: 0.0815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|████████████████████████| 254/254 [00:10<00:00, 24.73it/s, loss=3.0170, acc=0.0821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Loss: 3.0777, Train Acc: 0.0821, Val Acc: 0.0842\n",
      "  ✓ New best validation accuracy: 0.0842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|████████████████████████| 254/254 [00:10<00:00, 24.56it/s, loss=2.9985, acc=0.0858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Loss: 3.0793, Train Acc: 0.0858, Val Acc: 0.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|████████████████████████| 254/254 [00:11<00:00, 22.70it/s, loss=3.3252, acc=0.0843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Loss: 3.0835, Train Acc: 0.0843, Val Acc: 0.0836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|████████████████████████| 254/254 [00:10<00:00, 25.28it/s, loss=3.1625, acc=0.0802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Loss: 3.0808, Train Acc: 0.0802, Val Acc: 0.0768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|████████████████████████| 254/254 [00:10<00:00, 25.01it/s, loss=2.9709, acc=0.0914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Loss: 3.0671, Train Acc: 0.0914, Val Acc: 0.1012\n",
      "  ✓ New best validation accuracy: 0.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|████████████████████████| 254/254 [00:10<00:00, 24.75it/s, loss=3.1130, acc=0.0887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Loss: 3.0719, Train Acc: 0.0887, Val Acc: 0.0944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|████████████████████████| 254/254 [00:09<00:00, 25.43it/s, loss=3.0299, acc=0.0945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Loss: 3.0679, Train Acc: 0.0945, Val Acc: 0.0876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|███████████████████████| 254/254 [00:10<00:00, 25.34it/s, loss=2.9608, acc=0.0963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Loss: 3.0640, Train Acc: 0.0963, Val Acc: 0.1019\n",
      "  ✓ New best validation accuracy: 0.1019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|███████████████████████| 254/254 [00:09<00:00, 26.00it/s, loss=3.1010, acc=0.1062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Loss: 3.0588, Train Acc: 0.1062, Val Acc: 0.0958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|███████████████████████| 254/254 [00:10<00:00, 25.15it/s, loss=3.0940, acc=0.1072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Loss: 3.0569, Train Acc: 0.1072, Val Acc: 0.1053\n",
      "  ✓ New best validation accuracy: 0.1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|███████████████████████| 254/254 [00:09<00:00, 25.89it/s, loss=3.1552, acc=0.1078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Loss: 3.0522, Train Acc: 0.1078, Val Acc: 0.0965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|███████████████████████| 254/254 [00:09<00:00, 25.74it/s, loss=3.0100, acc=0.1005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Loss: 3.0550, Train Acc: 0.1005, Val Acc: 0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|███████████████████████| 254/254 [00:09<00:00, 26.12it/s, loss=2.8487, acc=0.1061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Loss: 3.0401, Train Acc: 0.1061, Val Acc: 0.0951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|███████████████████████| 254/254 [00:09<00:00, 26.10it/s, loss=2.9209, acc=0.1150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Loss: 3.0425, Train Acc: 0.1150, Val Acc: 0.0985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|███████████████████████| 254/254 [00:09<00:00, 26.12it/s, loss=2.9583, acc=0.1175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Loss: 3.0381, Train Acc: 0.1175, Val Acc: 0.1080\n",
      "  ✓ New best validation accuracy: 0.1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|███████████████████████| 254/254 [00:09<00:00, 25.79it/s, loss=2.9338, acc=0.1171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Loss: 3.0373, Train Acc: 0.1171, Val Acc: 0.1101\n",
      "  ✓ New best validation accuracy: 0.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|███████████████████████| 254/254 [00:09<00:00, 25.70it/s, loss=3.1555, acc=0.1161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - Loss: 3.0384, Train Acc: 0.1161, Val Acc: 0.0958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|███████████████████████| 254/254 [00:10<00:00, 23.31it/s, loss=2.9174, acc=0.1134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - Loss: 3.0434, Train Acc: 0.1134, Val Acc: 0.1046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|███████████████████████| 254/254 [00:10<00:00, 23.92it/s, loss=2.9708, acc=0.1147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - Loss: 3.0354, Train Acc: 0.1147, Val Acc: 0.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|███████████████████████| 254/254 [00:10<00:00, 24.03it/s, loss=2.9643, acc=0.1169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - Loss: 3.0284, Train Acc: 0.1169, Val Acc: 0.1087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|███████████████████████| 254/254 [00:10<00:00, 24.59it/s, loss=3.0747, acc=0.1204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - Loss: 3.0256, Train Acc: 0.1204, Val Acc: 0.1128\n",
      "  ✓ New best validation accuracy: 0.1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|███████████████████████| 254/254 [00:10<00:00, 24.64it/s, loss=3.1437, acc=0.1198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - Loss: 3.0250, Train Acc: 0.1198, Val Acc: 0.1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|███████████████████████| 254/254 [00:10<00:00, 25.00it/s, loss=2.9169, acc=0.1168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - Loss: 3.0310, Train Acc: 0.1168, Val Acc: 0.1019\n",
      "\n",
      "Evaluating Intent Classifier on Test Set...\n",
      "Test Intent Accuracy: 0.1161\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: Training Slot Filler (RNN)\n",
      "================================================================================\n",
      "Training Slot Filler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|████████████████████████████████████| 254/254 [00:11<00:00, 21.18it/s, loss=0.6002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Loss: 0.8723, Val F1: 0.8467, Val Acc: 0.8874\n",
      "  ✓ New best validation F1: 0.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|████████████████████████████████████| 254/254 [00:12<00:00, 19.74it/s, loss=0.6367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Loss: 0.6828, Val F1: 0.8369, Val Acc: 0.8664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|████████████████████████████████████| 254/254 [00:11<00:00, 21.23it/s, loss=0.6337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Loss: 0.6643, Val F1: 0.8517, Val Acc: 0.8994\n",
      "  ✓ New best validation F1: 0.8517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|████████████████████████████████████| 254/254 [00:12<00:00, 20.91it/s, loss=0.6410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Loss: 0.6457, Val F1: 0.8517, Val Acc: 0.8994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|████████████████████████████████████| 254/254 [00:12<00:00, 19.72it/s, loss=0.6361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Loss: 0.6455, Val F1: 0.8646, Val Acc: 0.9019\n",
      "  ✓ New best validation F1: 0.8646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|████████████████████████████████████| 254/254 [00:12<00:00, 20.04it/s, loss=0.5821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Loss: 0.6197, Val F1: 0.8517, Val Acc: 0.8994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|████████████████████████████████████| 254/254 [00:12<00:00, 20.67it/s, loss=0.6152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Loss: 0.6187, Val F1: 0.8517, Val Acc: 0.8994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|████████████████████████████████████| 254/254 [00:13<00:00, 19.05it/s, loss=0.6115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Loss: 0.6151, Val F1: 0.8517, Val Acc: 0.8994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|████████████████████████████████████| 254/254 [00:14<00:00, 17.61it/s, loss=0.6579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Loss: 0.6132, Val F1: 0.8578, Val Acc: 0.9004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|███████████████████████████████████| 254/254 [00:13<00:00, 18.75it/s, loss=0.6105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Loss: 0.6100, Val F1: 0.8517, Val Acc: 0.8994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|███████████████████████████████████| 254/254 [00:13<00:00, 18.96it/s, loss=0.6800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Loss: 0.6097, Val F1: 0.8719, Val Acc: 0.9004\n",
      "  ✓ New best validation F1: 0.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|███████████████████████████████████| 254/254 [00:13<00:00, 18.29it/s, loss=0.6732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Loss: 0.6058, Val F1: 0.8517, Val Acc: 0.8994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|███████████████████████████████████| 254/254 [00:12<00:00, 20.96it/s, loss=0.5896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Loss: 0.6057, Val F1: 0.8685, Val Acc: 0.9031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|███████████████████████████████████| 254/254 [00:13<00:00, 19.17it/s, loss=0.5683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Loss: 0.6018, Val F1: 0.8517, Val Acc: 0.8994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|███████████████████████████████████| 254/254 [00:11<00:00, 21.28it/s, loss=0.5657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Loss: 0.6036, Val F1: 0.8609, Val Acc: 0.8997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|███████████████████████████████████| 254/254 [00:11<00:00, 21.20it/s, loss=0.5565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Loss: 0.6000, Val F1: 0.8727, Val Acc: 0.9014\n",
      "  ✓ New best validation F1: 0.8727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|███████████████████████████████████| 254/254 [00:12<00:00, 20.06it/s, loss=0.6296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Loss: 0.5988, Val F1: 0.8744, Val Acc: 0.9023\n",
      "  ✓ New best validation F1: 0.8744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|███████████████████████████████████| 254/254 [00:12<00:00, 20.50it/s, loss=0.6152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Loss: 0.5961, Val F1: 0.8533, Val Acc: 0.8998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|███████████████████████████████████| 254/254 [00:12<00:00, 20.69it/s, loss=0.6040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - Loss: 0.5983, Val F1: 0.8588, Val Acc: 0.9002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|███████████████████████████████████| 254/254 [00:12<00:00, 20.48it/s, loss=0.5628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - Loss: 0.5939, Val F1: 0.8521, Val Acc: 0.8994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|███████████████████████████████████| 254/254 [00:11<00:00, 21.24it/s, loss=0.6178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - Loss: 0.5939, Val F1: 0.8679, Val Acc: 0.9027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|███████████████████████████████████| 254/254 [00:13<00:00, 18.94it/s, loss=0.5448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - Loss: 0.5925, Val F1: 0.8525, Val Acc: 0.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|███████████████████████████████████| 254/254 [00:12<00:00, 19.70it/s, loss=0.5867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - Loss: 0.5920, Val F1: 0.8546, Val Acc: 0.9005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|███████████████████████████████████| 254/254 [00:13<00:00, 19.38it/s, loss=0.5485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - Loss: 0.5911, Val F1: 0.8661, Val Acc: 0.9031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|███████████████████████████████████| 254/254 [00:12<00:00, 21.15it/s, loss=0.6734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - Loss: 0.5892, Val F1: 0.8559, Val Acc: 0.9011\n",
      "\n",
      "Evaluating Slot Filler on Test Set...\n",
      "Test Slot Accuracy: 0.9007\n",
      "Test Slot Precision: 0.8152\n",
      "Test Slot Recall: 0.9007\n",
      "Test Slot F1: 0.8552\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "Intent Classification - Test Accuracy: 0.1161\n",
      "Slot Filling - Test F1: 0.8552\n",
      "Slot Filling - Test Accuracy: 0.9007\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def parse_atis_sample(sample):\n",
    "    \"\"\"Parse ATIS JSON sample\"\"\"\n",
    "    if 'tokens' in sample:\n",
    "        tokens = sample['tokens']\n",
    "    elif 'text' in sample:\n",
    "        tokens = sample['text'].split() if isinstance(sample['text'], str) else sample['text']\n",
    "    else:\n",
    "        tokens = []\n",
    "\n",
    "    intent = sample.get('intent', sample.get('label', sample.get('intent_label', '')))\n",
    "\n",
    "    if 'slots' in sample:\n",
    "        slots = sample['slots']\n",
    "    elif 'slot_labels' in sample:\n",
    "        slots = sample['slot_labels']\n",
    "    else:\n",
    "        slots = ['O'] * len(tokens)\n",
    "\n",
    "    return tokens, intent, slots\n",
    "\n",
    "def parse_slurp_sample(sample):\n",
    "    \"\"\"Parse SLURP JSON sample - handles multiple formats\"\"\"\n",
    "    # Try to get tokens from various possible fields\n",
    "    tokens = []\n",
    "    if 'tokens' in sample:\n",
    "        tokens = sample['tokens']\n",
    "    elif 'sentence' in sample:\n",
    "        # If sentence is a string, split it\n",
    "        sentence = sample['sentence']\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = sentence.split()\n",
    "        else:\n",
    "            tokens = sentence\n",
    "    elif 'text' in sample:\n",
    "        text = sample['text']\n",
    "        if isinstance(text, str):\n",
    "            tokens = text.split()\n",
    "        else:\n",
    "            tokens = text\n",
    "    \n",
    "    # Try to get intent from various possible fields\n",
    "    scenario = sample.get('scenario', '')\n",
    "    action = sample.get('action', '')\n",
    "    \n",
    "    # Check if intent is directly provided\n",
    "    if 'intent' in sample:\n",
    "        intent = sample['intent']\n",
    "    elif scenario and action:\n",
    "        intent = f\"{scenario}_{action}\"\n",
    "    elif scenario:\n",
    "        intent = scenario\n",
    "    elif action:\n",
    "        intent = action\n",
    "    else:\n",
    "        intent = ''\n",
    "    \n",
    "    # Get entities/slots\n",
    "    entities = sample.get('entities', [])\n",
    "    slots = ['O'] * len(tokens)\n",
    "\n",
    "    for entity in entities:\n",
    "        slot_type = entity.get('type', 'entity')\n",
    "        start = entity.get('start', 0)\n",
    "        end = entity.get('end', 0)\n",
    "\n",
    "        if start < len(slots):\n",
    "            slots[start] = f'B-{slot_type}'\n",
    "        for i in range(start + 1, min(end, len(slots))):\n",
    "            slots[i] = f'I-{slot_type}'\n",
    "\n",
    "    return tokens, intent, slots\n",
    "\n",
    "def load_json_dataset(file_path, dataset_type='atis'):\n",
    "    \"\"\"Load dataset from JSON Lines (.jsonl) file\"\"\"\n",
    "    print(f\"Loading {file_path}...\")\n",
    "\n",
    "    parsed_data = []\n",
    "    parse_fn = parse_atis_sample if dataset_type == 'atis' else parse_slurp_sample\n",
    "    error_count = 0\n",
    "    sample_shown = False\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line:  # Skip empty lines\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    sample = json.loads(line)\n",
    "                    \n",
    "                    # Show first sample for debugging\n",
    "                    if not sample_shown and line_num == 1:\n",
    "                        print(f\"\\n  First sample structure: {list(sample.keys())}\")\n",
    "                        print(f\"  Sample preview: {str(sample)[:200]}...\")\n",
    "                        sample_shown = True\n",
    "                    \n",
    "                    tokens, intent, slots = parse_fn(sample)\n",
    "                    \n",
    "                    # Validate the sample\n",
    "                    if tokens and intent:\n",
    "                        # Ensure slots match tokens length\n",
    "                        if len(slots) != len(tokens):\n",
    "                            slots = slots[:len(tokens)] + ['O'] * max(0, len(tokens) - len(slots))\n",
    "                        parsed_data.append((tokens, intent, slots))\n",
    "                    else:\n",
    "                        error_count += 1\n",
    "                        if error_count <= 3:  # Show first 3 errors with details\n",
    "                            print(f\"  Warning line {line_num}: Invalid sample\")\n",
    "                            print(f\"    - Has tokens: {bool(tokens)} (length: {len(tokens)})\")\n",
    "                            print(f\"    - Has intent: {bool(intent)} (value: '{intent}')\")\n",
    "                            \n",
    "                except json.JSONDecodeError as e:\n",
    "                    error_count += 1\n",
    "                    if error_count <= 3:\n",
    "                        print(f\"  Error line {line_num}: JSON decode error - {e}\")\n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    if error_count <= 3:\n",
    "                        print(f\"  Error line {line_num}: {e}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found - {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading file: {e}\")\n",
    "        return []\n",
    "\n",
    "    if error_count > 3:\n",
    "        print(f\"  ... and {error_count - 3} more errors\")\n",
    "    \n",
    "    print(f\"Loaded {len(parsed_data)} valid samples\")\n",
    "    \n",
    "    if len(parsed_data) == 0:\n",
    "        print(f\"\\n⚠️  WARNING: No valid samples loaded from {file_path}!\")\n",
    "        print(\"\\nPlease check the file format. The parser is looking for:\")\n",
    "        if dataset_type == 'slurp':\n",
    "            print(\"  - 'tokens' OR 'sentence' OR 'text': the input words\")\n",
    "            print(\"  - 'scenario' + 'action' OR 'intent': the intent label\")\n",
    "            print(\"  - 'entities': entity annotations (optional)\")\n",
    "        else:\n",
    "            print(\"  - 'tokens' or 'text': words/sentence\")\n",
    "            print(\"  - 'intent': intent label\")\n",
    "            print(\"  - 'slots': slot labels\")\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "def build_vocab_and_mappings(train_data, val_data, min_freq=1):\n",
    "    \"\"\"Build vocabulary and label mappings from data\"\"\"\n",
    "    print(\"\\nBuilding vocabulary and mappings...\")\n",
    "\n",
    "    if len(train_data) == 0 and len(val_data) == 0:\n",
    "        print(\"ERROR: No training or validation data available!\")\n",
    "        return {'<PAD>': 0, '<UNK>': 1}, {}, {}\n",
    "\n",
    "    # Collect all tokens, intents, and slots\n",
    "    all_tokens = []\n",
    "    all_intents = []\n",
    "    all_slots = []\n",
    "\n",
    "    for tokens, intent, slots in train_data + val_data:\n",
    "        all_tokens.extend([t.lower() for t in tokens])\n",
    "        all_intents.append(intent)\n",
    "        all_slots.extend(slots)\n",
    "\n",
    "    # Build vocabulary\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    idx = 2\n",
    "    for token, freq in token_counter.most_common():\n",
    "        if freq >= min_freq:\n",
    "            vocab[token] = idx\n",
    "            idx += 1\n",
    "\n",
    "    # Build intent mapping\n",
    "    unique_intents = sorted(set(all_intents))\n",
    "    intent2idx = {intent: idx for idx, intent in enumerate(unique_intents)}\n",
    "\n",
    "    # Build slot mapping\n",
    "    unique_slots = sorted(set(all_slots))\n",
    "    slot2idx = {slot: idx for idx, slot in enumerate(unique_slots)}\n",
    "\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    print(f\"Number of intents: {len(intent2idx)}\")\n",
    "    print(f\"Number of slots: {len(slot2idx)}\")\n",
    "\n",
    "    return vocab, intent2idx, slot2idx\n",
    "\n",
    "def convert_to_ids(data, vocab, intent2idx, slot2idx):\n",
    "    \"\"\"Convert tokens and labels to IDs\"\"\"\n",
    "    processed = []\n",
    "\n",
    "    for tokens, intent, slots in data:\n",
    "        # Convert tokens to IDs\n",
    "        input_ids = [vocab.get(t.lower(), vocab['<UNK>']) for t in tokens]\n",
    "\n",
    "        # Convert intent to ID\n",
    "        intent_id = intent2idx.get(intent, 0)\n",
    "\n",
    "        # Convert slots to IDs\n",
    "        slot_ids = [slot2idx.get(s, 0) for s in slots]\n",
    "\n",
    "        processed.append({\n",
    "            'input_ids': input_ids,\n",
    "            'intent': intent_id,\n",
    "            'slots': slot_ids\n",
    "        })\n",
    "\n",
    "    return processed\n",
    "\n",
    "# ============================================================================\n",
    "# ACTIVATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# ============================================================================\n",
    "# RNN CELL FROM SCRATCH\n",
    "# ============================================================================\n",
    "\n",
    "class RNNCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        limit_ih = np.sqrt(6 / (input_size + hidden_size))\n",
    "        limit_hh = np.sqrt(6 / (hidden_size + hidden_size))\n",
    "\n",
    "        self.W_ih = np.random.uniform(-limit_ih, limit_ih, (input_size, hidden_size))\n",
    "        self.W_hh = np.random.uniform(-limit_hh, limit_hh, (hidden_size, hidden_size))\n",
    "        self.b_h = np.zeros((1, hidden_size))\n",
    "\n",
    "        self.m_W_ih = np.zeros_like(self.W_ih)\n",
    "        self.v_W_ih = np.zeros_like(self.W_ih)\n",
    "        self.m_W_hh = np.zeros_like(self.W_hh)\n",
    "        self.v_W_hh = np.zeros_like(self.W_hh)\n",
    "        self.m_b_h = np.zeros_like(self.b_h)\n",
    "        self.v_b_h = np.zeros_like(self.b_h)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        self.x = x\n",
    "        self.h_prev = h_prev\n",
    "        self.h = tanh(x @ self.W_ih + h_prev @ self.W_hh + self.b_h)\n",
    "        return self.h\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        dh_raw = dh_next * tanh_derivative(self.h)\n",
    "\n",
    "        self.dW_ih = self.x.T @ dh_raw\n",
    "        self.dW_hh = self.h_prev.T @ dh_raw\n",
    "        self.db_h = np.sum(dh_raw, axis=0, keepdims=True)\n",
    "\n",
    "        dx = dh_raw @ self.W_ih.T\n",
    "        dh_prev = dh_raw @ self.W_hh.T\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "# ============================================================================\n",
    "# BIDIRECTIONAL RNN LAYER\n",
    "# ============================================================================\n",
    "\n",
    "class BiRNN:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forward_cells = []\n",
    "        self.backward_cells = []\n",
    "        self.max_cells = 0\n",
    "\n",
    "    def forward(self, X, seq_lengths=None):\n",
    "        batch_size, max_seq_len, _ = X.shape\n",
    "\n",
    "        # Dynamically create cells if needed\n",
    "        while len(self.forward_cells) < max_seq_len:\n",
    "            self.forward_cells.append(RNNCell(self.input_size, self.hidden_size))\n",
    "            self.backward_cells.append(RNNCell(self.input_size, self.hidden_size))\n",
    "        \n",
    "        self.max_cells = max(self.max_cells, max_seq_len)\n",
    "\n",
    "        h_forward = np.zeros((batch_size, self.hidden_size))\n",
    "        forward_hiddens = []\n",
    "\n",
    "        for t in range(max_seq_len):\n",
    "            h_forward = self.forward_cells[t].forward(X[:, t, :], h_forward)\n",
    "            forward_hiddens.append(h_forward)\n",
    "\n",
    "        h_backward = np.zeros((batch_size, self.hidden_size))\n",
    "        backward_hiddens = []\n",
    "\n",
    "        for t in range(max_seq_len - 1, -1, -1):\n",
    "            h_backward = self.backward_cells[t].forward(X[:, t, :], h_backward)\n",
    "            backward_hiddens.insert(0, h_backward)\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(max_seq_len):\n",
    "            outputs.append(np.concatenate([forward_hiddens[t], backward_hiddens[t]], axis=1))\n",
    "\n",
    "        outputs = np.stack(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, doutputs):\n",
    "        batch_size, max_seq_len, _ = doutputs.shape\n",
    "\n",
    "        dforward = doutputs[:, :, :self.hidden_size]\n",
    "        dbackward = doutputs[:, :, self.hidden_size:]\n",
    "\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
    "        for t in range(max_seq_len - 1, -1, -1):\n",
    "            dh = dforward[:, t, :] + dh_next\n",
    "            _, dh_next = self.forward_cells[t].backward(dh)\n",
    "\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
    "        for t in range(max_seq_len):\n",
    "            dh = dbackward[:, t, :] + dh_next\n",
    "            _, dh_next = self.backward_cells[t].backward(dh)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# LSTM CELL FROM SCRATCH\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# LSTM CELL FROM SCRATCH\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# LSTM CELL FROM SCRATCH\n",
    "# ============================================================================\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Xavier initialization\n",
    "        limit_ih = np.sqrt(6 / (input_size + hidden_size))\n",
    "        limit_hh = np.sqrt(6 / (hidden_size + hidden_size))\n",
    "\n",
    "        # Forget gate parameters\n",
    "        self.W_if = np.random.uniform(-limit_ih, limit_ih, (input_size, hidden_size))\n",
    "        self.W_hf = np.random.uniform(-limit_hh, limit_hh, (hidden_size, hidden_size))\n",
    "        self.b_f = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Input gate parameters\n",
    "        self.W_ii = np.random.uniform(-limit_ih, limit_ih, (input_size, hidden_size))\n",
    "        self.W_hi = np.random.uniform(-limit_hh, limit_hh, (hidden_size, hidden_size))\n",
    "        self.b_i = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Cell gate parameters\n",
    "        self.W_ig = np.random.uniform(-limit_ih, limit_ih, (input_size, hidden_size))\n",
    "        self.W_hg = np.random.uniform(-limit_hh, limit_hh, (hidden_size, hidden_size))\n",
    "        self.b_g = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Output gate parameters\n",
    "        self.W_io = np.random.uniform(-limit_ih, limit_ih, (input_size, hidden_size))\n",
    "        self.W_ho = np.random.uniform(-limit_hh, limit_hh, (hidden_size, hidden_size))\n",
    "        self.b_o = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Initialize Adam optimizer parameters for all weights\n",
    "        self.m_W_if = np.zeros_like(self.W_if)\n",
    "        self.v_W_if = np.zeros_like(self.W_if)\n",
    "        self.m_W_hf = np.zeros_like(self.W_hf)\n",
    "        self.v_W_hf = np.zeros_like(self.W_hf)\n",
    "        self.m_b_f = np.zeros_like(self.b_f)\n",
    "        self.v_b_f = np.zeros_like(self.b_f)\n",
    "\n",
    "        self.m_W_ii = np.zeros_like(self.W_ii)\n",
    "        self.v_W_ii = np.zeros_like(self.W_ii)\n",
    "        self.m_W_hi = np.zeros_like(self.W_hi)\n",
    "        self.v_W_hi = np.zeros_like(self.W_hi)\n",
    "        self.m_b_i = np.zeros_like(self.b_i)\n",
    "        self.v_b_i = np.zeros_like(self.b_i)\n",
    "\n",
    "        self.m_W_ig = np.zeros_like(self.W_ig)\n",
    "        self.v_W_ig = np.zeros_like(self.W_ig)\n",
    "        self.m_W_hg = np.zeros_like(self.W_hg)\n",
    "        self.v_W_hg = np.zeros_like(self.W_hg)\n",
    "        self.m_b_g = np.zeros_like(self.b_g)\n",
    "        self.v_b_g = np.zeros_like(self.b_g)\n",
    "\n",
    "        self.m_W_io = np.zeros_like(self.W_io)\n",
    "        self.v_W_io = np.zeros_like(self.W_io)\n",
    "        self.m_W_ho = np.zeros_like(self.W_ho)\n",
    "        self.v_W_ho = np.zeros_like(self.W_ho)\n",
    "        self.m_b_o = np.zeros_like(self.b_o)\n",
    "        self.v_b_o = np.zeros_like(self.b_o)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        self.x = x\n",
    "        self.h_prev = h_prev\n",
    "        self.c_prev = c_prev\n",
    "\n",
    "        # Forget gate\n",
    "        self.f_gate = self.sigmoid(x @ self.W_if + h_prev @ self.W_hf + self.b_f)\n",
    "        \n",
    "        # Input gate\n",
    "        self.i_gate = self.sigmoid(x @ self.W_ii + h_prev @ self.W_hi + self.b_i)\n",
    "        \n",
    "        # Cell gate (candidate)\n",
    "        self.g_gate = self.tanh(x @ self.W_ig + h_prev @ self.W_hg + self.b_g)\n",
    "        \n",
    "        # Output gate\n",
    "        self.o_gate = self.sigmoid(x @ self.W_io + h_prev @ self.W_ho + self.b_o)\n",
    "\n",
    "        # New cell state\n",
    "        self.c = self.f_gate * c_prev + self.i_gate * self.g_gate\n",
    "        \n",
    "        # New hidden state\n",
    "        self.h = self.o_gate * self.tanh(self.c)\n",
    "\n",
    "        return self.h, self.c\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        # Gradient through output gate\n",
    "        dtanh_c = dh_next * self.o_gate\n",
    "        dc = dc_next + dtanh_c * (1 - self.tanh(self.c) ** 2)\n",
    "\n",
    "        # Gradient through output gate\n",
    "        do_raw = dh_next * self.tanh(self.c)\n",
    "        do = do_raw * self.o_gate * (1 - self.o_gate)\n",
    "\n",
    "        # Gradient through cell state\n",
    "        df_raw = dc * self.c_prev\n",
    "        df = df_raw * self.f_gate * (1 - self.f_gate)\n",
    "\n",
    "        di_raw = dc * self.g_gate\n",
    "        di = di_raw * self.i_gate * (1 - self.i_gate)\n",
    "\n",
    "        dg_raw = dc * self.i_gate\n",
    "        dg = dg_raw * (1 - self.g_gate ** 2)\n",
    "\n",
    "        # Compute weight gradients\n",
    "        self.dW_if = self.x.T @ df\n",
    "        self.dW_hf = self.h_prev.T @ df\n",
    "        self.db_f = np.sum(df, axis=0, keepdims=True)\n",
    "\n",
    "        self.dW_ii = self.x.T @ di\n",
    "        self.dW_hi = self.h_prev.T @ di\n",
    "        self.db_i = np.sum(di, axis=0, keepdims=True)\n",
    "\n",
    "        self.dW_ig = self.x.T @ dg\n",
    "        self.dW_hg = self.h_prev.T @ dg\n",
    "        self.db_g = np.sum(dg, axis=0, keepdims=True)\n",
    "\n",
    "        self.dW_io = self.x.T @ do\n",
    "        self.dW_ho = self.h_prev.T @ do\n",
    "        self.db_o = np.sum(do, axis=0, keepdims=True)\n",
    "\n",
    "        # Compute gradients for inputs\n",
    "        dx = (df @ self.W_if.T + di @ self.W_ii.T + \n",
    "              dg @ self.W_ig.T + do @ self.W_io.T)\n",
    "        \n",
    "        dh_prev = (df @ self.W_hf.T + di @ self.W_hi.T + \n",
    "                   dg @ self.W_hg.T + do @ self.W_ho.T)\n",
    "        \n",
    "        dc_prev = dc * self.f_gate\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BIDIRECTIONAL LSTM LAYER\n",
    "# ============================================================================\n",
    "\n",
    "class BiLSTM:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forward_cells = []\n",
    "        self.backward_cells = []\n",
    "        self.max_cells = 0\n",
    "\n",
    "    def forward(self, X, seq_lengths=None):\n",
    "        batch_size, max_seq_len, _ = X.shape\n",
    "\n",
    "        # Dynamically create cells if needed\n",
    "        while len(self.forward_cells) < max_seq_len:\n",
    "            self.forward_cells.append(LSTMCell(self.input_size, self.hidden_size))\n",
    "            self.backward_cells.append(LSTMCell(self.input_size, self.hidden_size))\n",
    "        \n",
    "        self.max_cells = max(self.max_cells, max_seq_len)\n",
    "\n",
    "        # Forward pass\n",
    "        h_forward = np.zeros((batch_size, self.hidden_size))\n",
    "        c_forward = np.zeros((batch_size, self.hidden_size))\n",
    "        forward_hiddens = []\n",
    "\n",
    "        for t in range(max_seq_len):\n",
    "            h_forward, c_forward = self.forward_cells[t].forward(\n",
    "                X[:, t, :], h_forward, c_forward\n",
    "            )\n",
    "            forward_hiddens.append(h_forward)\n",
    "\n",
    "        # Backward pass\n",
    "        h_backward = np.zeros((batch_size, self.hidden_size))\n",
    "        c_backward = np.zeros((batch_size, self.hidden_size))\n",
    "        backward_hiddens = []\n",
    "\n",
    "        for t in range(max_seq_len - 1, -1, -1):\n",
    "            h_backward, c_backward = self.backward_cells[t].forward(\n",
    "                X[:, t, :], h_backward, c_backward\n",
    "            )\n",
    "            backward_hiddens.insert(0, h_backward)\n",
    "\n",
    "        # Concatenate forward and backward outputs\n",
    "        outputs = []\n",
    "        for t in range(max_seq_len):\n",
    "            outputs.append(np.concatenate([forward_hiddens[t], backward_hiddens[t]], axis=1))\n",
    "\n",
    "        outputs = np.stack(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, doutputs):\n",
    "        batch_size, max_seq_len, _ = doutputs.shape\n",
    "\n",
    "        dforward = doutputs[:, :, :self.hidden_size]\n",
    "        dbackward = doutputs[:, :, self.hidden_size:]\n",
    "\n",
    "        # Backward pass through forward direction\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
    "        dc_next = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        for t in range(max_seq_len - 1, -1, -1):\n",
    "            dh = dforward[:, t, :] + dh_next\n",
    "            _, dh_next, dc_next = self.forward_cells[t].backward(dh, dc_next)\n",
    "\n",
    "        # Backward pass through backward direction\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
    "        dc_next = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        for t in range(max_seq_len):\n",
    "            dh = dbackward[:, t, :] + dh_next\n",
    "            _, dh_next, dc_next = self.backward_cells[t].backward(dh, dc_next)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTENT CLASSIFIER WITH LSTM\n",
    "# ============================================================================\n",
    "\n",
    "class IntentClassifierLSTM:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_intents):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_intents = num_intents\n",
    "\n",
    "        self.embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "        self.encoder = BiLSTM(embed_dim, hidden_dim)\n",
    "\n",
    "        self.W_out = np.random.randn(2 * hidden_dim, num_intents) * 0.01\n",
    "        self.b_out = np.zeros((1, num_intents))\n",
    "\n",
    "        self.m_W_out = np.zeros_like(self.W_out)\n",
    "        self.v_W_out = np.zeros_like(self.W_out)\n",
    "        self.m_b_out = np.zeros_like(self.b_out)\n",
    "        self.v_b_out = np.zeros_like(self.b_out)\n",
    "        self.m_emb = np.zeros_like(self.embeddings)\n",
    "        self.v_emb = np.zeros_like(self.embeddings)\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, input_ids, training=True):\n",
    "        self.input_ids = input_ids\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        self.embeds = self.embeddings[input_ids]\n",
    "\n",
    "        if training:\n",
    "            self.dropout_mask = (np.random.rand(*self.embeds.shape) > 0.3).astype(float) / 0.7\n",
    "            embeds_dropped = self.embeds * self.dropout_mask\n",
    "        else:\n",
    "            embeds_dropped = self.embeds\n",
    "\n",
    "        self.rnn_outputs = self.encoder.forward(embeds_dropped)\n",
    "        self.final_hidden = self.rnn_outputs[:, -1, :]\n",
    "        self.logits = self.final_hidden @ self.W_out + self.b_out\n",
    "\n",
    "        return self.logits\n",
    "\n",
    "    def backward(self, dloss):\n",
    "        batch_size = dloss.shape[0]\n",
    "\n",
    "        dW_out = self.final_hidden.T @ dloss\n",
    "        db_out = np.sum(dloss, axis=0, keepdims=True)\n",
    "\n",
    "        dfinal_hidden = dloss @ self.W_out.T\n",
    "\n",
    "        drnn_outputs = np.zeros_like(self.rnn_outputs)\n",
    "        drnn_outputs[:, -1, :] = dfinal_hidden\n",
    "\n",
    "        self.encoder.backward(drnn_outputs)\n",
    "\n",
    "        return dW_out, db_out\n",
    "\n",
    "    def update(self, dW_out, db_out, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.t += 1\n",
    "\n",
    "        # Update output layer\n",
    "        self.m_W_out = beta1 * self.m_W_out + (1 - beta1) * dW_out\n",
    "        self.v_W_out = beta2 * self.v_W_out + (1 - beta2) * (dW_out ** 2)\n",
    "        m_hat = self.m_W_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_W_out / (1 - beta2 ** self.t)\n",
    "        self.W_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        self.m_b_out = beta1 * self.m_b_out + (1 - beta1) * db_out\n",
    "        self.v_b_out = beta2 * self.v_b_out + (1 - beta2) * (db_out ** 2)\n",
    "        m_hat = self.m_b_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_b_out / (1 - beta2 ** self.t)\n",
    "        self.b_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        # Update LSTM cells (same as IntentClassifierLSTM)\n",
    "        for cell in self.encoder.forward_cells + self.encoder.backward_cells:\n",
    "            # Update forget gate weights\n",
    "            cell.m_W_if = beta1 * cell.m_W_if + (1 - beta1) * cell.dW_if\n",
    "            cell.v_W_if = beta2 * cell.v_W_if + (1 - beta2) * (cell.dW_if ** 2)\n",
    "            m_hat_if = cell.m_W_if / (1 - beta1 ** self.t)\n",
    "            v_hat_if = cell.v_W_if / (1 - beta2 ** self.t)\n",
    "            cell.W_if -= lr * m_hat_if / (np.sqrt(v_hat_if) + eps)\n",
    "\n",
    "            cell.m_W_hf = beta1 * cell.m_W_hf + (1 - beta1) * cell.dW_hf\n",
    "            cell.v_W_hf = beta2 * cell.v_W_hf + (1 - beta2) * (cell.dW_hf ** 2)\n",
    "            m_hat_hf = cell.m_W_hf / (1 - beta1 ** self.t)\n",
    "            v_hat_hf = cell.v_W_hf / (1 - beta2 ** self.t)\n",
    "            cell.W_hf -= lr * m_hat_hf / (np.sqrt(v_hat_hf) + eps)\n",
    "\n",
    "            cell.m_b_f = beta1 * cell.m_b_f + (1 - beta1) * cell.db_f\n",
    "            cell.v_b_f = beta2 * cell.v_b_f + (1 - beta2) * (cell.db_f ** 2)\n",
    "            m_hat_bf = cell.m_b_f / (1 - beta1 ** self.t)\n",
    "            v_hat_bf = cell.v_b_f / (1 - beta2 ** self.t)\n",
    "            cell.b_f -= lr * m_hat_bf / (np.sqrt(v_hat_bf) + eps)\n",
    "\n",
    "            # Update input gate weights\n",
    "            cell.m_W_ii = beta1 * cell.m_W_ii + (1 - beta1) * cell.dW_ii\n",
    "            cell.v_W_ii = beta2 * cell.v_W_ii + (1 - beta2) * (cell.dW_ii ** 2)\n",
    "            m_hat_ii = cell.m_W_ii / (1 - beta1 ** self.t)\n",
    "            v_hat_ii = cell.v_W_ii / (1 - beta2 ** self.t)\n",
    "            cell.W_ii -= lr * m_hat_ii / (np.sqrt(v_hat_ii) + eps)\n",
    "\n",
    "            cell.m_W_hi = beta1 * cell.m_W_hi + (1 - beta1) * cell.dW_hi\n",
    "            cell.v_W_hi = beta2 * cell.v_W_hi + (1 - beta2) * (cell.dW_hi ** 2)\n",
    "            m_hat_hi = cell.m_W_hi / (1 - beta1 ** self.t)\n",
    "            v_hat_hi = cell.v_W_hi / (1 - beta2 ** self.t)\n",
    "            cell.W_hi -= lr * m_hat_hi / (np.sqrt(v_hat_hi) + eps)\n",
    "\n",
    "            cell.m_b_i = beta1 * cell.m_b_i + (1 - beta1) * cell.db_i\n",
    "            cell.v_b_i = beta2 * cell.v_b_i + (1 - beta2) * (cell.db_i ** 2)\n",
    "            m_hat_bi = cell.m_b_i / (1 - beta1 ** self.t)\n",
    "            v_hat_bi = cell.v_b_i / (1 - beta2 ** self.t)\n",
    "            cell.b_i -= lr * m_hat_bi / (np.sqrt(v_hat_bi) + eps)\n",
    "\n",
    "            # Update cell gate weights\n",
    "            cell.m_W_ig = beta1 * cell.m_W_ig + (1 - beta1) * cell.dW_ig\n",
    "            cell.v_W_ig = beta2 * cell.v_W_ig + (1 - beta2) * (cell.dW_ig ** 2)\n",
    "            m_hat_ig = cell.m_W_ig / (1 - beta1 ** self.t)\n",
    "            v_hat_ig = cell.v_W_ig / (1 - beta2 ** self.t)\n",
    "            cell.W_ig -= lr * m_hat_ig / (np.sqrt(v_hat_ig) + eps)\n",
    "\n",
    "            cell.m_W_hg = beta1 * cell.m_W_hg + (1 - beta1) * cell.dW_hg\n",
    "            cell.v_W_hg = beta2 * cell.v_W_hg + (1 - beta2) * (cell.dW_hg ** 2)\n",
    "            m_hat_hg = cell.m_W_hg / (1 - beta1 ** self.t)\n",
    "            v_hat_hg = cell.v_W_hg / (1 - beta2 ** self.t)\n",
    "            cell.W_hg -= lr * m_hat_hg / (np.sqrt(v_hat_hg) + eps)\n",
    "\n",
    "            cell.m_b_g = beta1 * cell.m_b_g + (1 - beta1) * cell.db_g\n",
    "            cell.v_b_g = beta2 * cell.v_b_g + (1 - beta2) * (cell.db_g ** 2)\n",
    "            m_hat_bg = cell.m_b_g / (1 - beta1 ** self.t)\n",
    "            v_hat_bg = cell.v_b_g / (1 - beta2 ** self.t)\n",
    "            cell.b_g -= lr * m_hat_bg / (np.sqrt(v_hat_bg) + eps)\n",
    "\n",
    "            # Update output gate weights\n",
    "            cell.m_W_io = beta1 * cell.m_W_io + (1 - beta1) * cell.dW_io\n",
    "            cell.v_W_io = beta2 * cell.v_W_io + (1 - beta2) * (cell.dW_io ** 2)\n",
    "            m_hat_io = cell.m_W_io / (1 - beta1 ** self.t)\n",
    "            v_hat_io = cell.v_W_io / (1 - beta2 ** self.t)\n",
    "            cell.W_io -= lr * m_hat_io / (np.sqrt(v_hat_io) + eps)\n",
    "\n",
    "            cell.m_W_ho = beta1 * cell.m_W_ho + (1 - beta1) * cell.dW_ho\n",
    "            cell.v_W_ho = beta2 * cell.v_W_ho + (1 - beta2) * (cell.dW_ho ** 2)\n",
    "            m_hat_ho = cell.m_W_ho / (1 - beta1 ** self.t)\n",
    "            v_hat_ho = cell.v_W_ho / (1 - beta2 ** self.t)\n",
    "            cell.W_ho -= lr * m_hat_ho / (np.sqrt(v_hat_ho) + eps)\n",
    "\n",
    "            cell.m_b_o = beta1 * cell.m_b_o + (1 - beta1) * cell.db_o\n",
    "            cell.v_b_o = beta2 * cell.v_b_o + (1 - beta2) * (cell.db_o ** 2)\n",
    "            m_hat_bo = cell.m_b_o / (1 - beta1 ** self.t)\n",
    "            v_hat_bo = cell.v_b_o / (1 - beta2 ** self.t)\n",
    "            cell.b_o -= lr * m_hat_bo / (np.sqrt(v_hat_bo) + eps)\n",
    "\n",
    "    def predict(self, input_ids, intent_ids):\n",
    "        logits = self.forward(input_ids, intent_ids, training=False)\n",
    "        return np.argmax(logits, axis=2)\n",
    "\n",
    "            \n",
    "\n",
    "    def predict(self, input_ids):\n",
    "        logits = self.forward(input_ids, training=False)\n",
    "        return np.argmax(logits, axis=1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SLOT FILLING MODEL WITH LSTM\n",
    "# ============================================================================\n",
    "\n",
    "class SlotFillingWithIntentLSTM:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_slots, num_intents):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_slots = num_slots\n",
    "        self.num_intents = num_intents\n",
    "\n",
    "        self.word_embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "        self.intent_embeddings = np.random.randn(num_intents, 32) * 0.01\n",
    "\n",
    "        self.encoder = BiLSTM(embed_dim + 32, hidden_dim)\n",
    "\n",
    "        self.W_out = np.random.randn(2 * hidden_dim, num_slots) * 0.01\n",
    "        self.b_out = np.zeros((1, num_slots))\n",
    "\n",
    "        self.m_W_out = np.zeros_like(self.W_out)\n",
    "        self.v_W_out = np.zeros_like(self.W_out)\n",
    "        self.m_b_out = np.zeros_like(self.b_out)\n",
    "        self.v_b_out = np.zeros_like(self.b_out)\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, input_ids, intent_ids, training=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.intent_ids = intent_ids\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        self.word_embeds = self.word_embeddings[input_ids]\n",
    "\n",
    "        intent_embeds = self.intent_embeddings[intent_ids]\n",
    "        self.intent_embeds = np.repeat(intent_embeds[:, np.newaxis, :], seq_len, axis=1)\n",
    "\n",
    "        self.combined = np.concatenate([self.word_embeds, self.intent_embeds], axis=2)\n",
    "\n",
    "        if training:\n",
    "            self.dropout_mask = (np.random.rand(*self.combined.shape) > 0.3).astype(float) / 0.7\n",
    "            combined_dropped = self.combined * self.dropout_mask\n",
    "        else:\n",
    "            combined_dropped = self.combined\n",
    "\n",
    "        self.rnn_outputs = self.encoder.forward(combined_dropped)\n",
    "\n",
    "        batch_size, seq_len, hidden_size = self.rnn_outputs.shape\n",
    "        rnn_flat = self.rnn_outputs.reshape(-1, hidden_size)\n",
    "        logits_flat = rnn_flat @ self.W_out + self.b_out\n",
    "        self.logits = logits_flat.reshape(batch_size, seq_len, self.num_slots)\n",
    "\n",
    "        return self.logits\n",
    "\n",
    "    def backward(self, dloss):\n",
    "        batch_size, seq_len, _ = dloss.shape\n",
    "        hidden_size = 2 * self.hidden_dim\n",
    "\n",
    "        dloss_flat = dloss.reshape(-1, self.num_slots)\n",
    "        rnn_flat = self.rnn_outputs.reshape(-1, hidden_size)\n",
    "\n",
    "        dW_out = rnn_flat.T @ dloss_flat\n",
    "        db_out = np.sum(dloss_flat, axis=0, keepdims=True)\n",
    "\n",
    "        drnn_flat = dloss_flat @ self.W_out.T\n",
    "        drnn_outputs = drnn_flat.reshape(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        self.encoder.backward(drnn_outputs)\n",
    "\n",
    "        return dW_out, db_out\n",
    "\n",
    "    def update(self, dW_out, db_out, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.t += 1\n",
    "\n",
    "        # Update output layer\n",
    "        self.m_W_out = beta1 * self.m_W_out + (1 - beta1) * dW_out\n",
    "        self.v_W_out = beta2 * self.v_W_out + (1 - beta2) * (dW_out ** 2)\n",
    "        m_hat = self.m_W_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_W_out / (1 - beta2 ** self.t)\n",
    "        self.W_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        self.m_b_out = beta1 * self.m_b_out + (1 - beta1) * db_out\n",
    "        self.v_b_out = beta2 * self.v_b_out + (1 - beta2) * (db_out ** 2)\n",
    "        m_hat = self.m_b_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_b_out / (1 - beta2 ** self.t)\n",
    "        self.b_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        # Update LSTM cells\n",
    "        for cell in self.encoder.forward_cells + self.encoder.backward_cells:\n",
    "            # Update forget gate weights\n",
    "            cell.m_W_if = beta1 * cell.m_W_if + (1 - beta1) * cell.dW_if\n",
    "            cell.v_W_if = beta2 * cell.v_W_if + (1 - beta2) * (cell.dW_if ** 2)\n",
    "            m_hat_if = cell.m_W_if / (1 - beta1 ** self.t)\n",
    "            v_hat_if = cell.v_W_if / (1 - beta2 ** self.t)\n",
    "            cell.W_if -= lr * m_hat_if / (np.sqrt(v_hat_if) + eps)\n",
    "\n",
    "            cell.m_W_hf = beta1 * cell.m_W_hf + (1 - beta1) * cell.dW_hf\n",
    "            cell.v_W_hf = beta2 * cell.v_W_hf + (1 - beta2) * (cell.dW_hf ** 2)\n",
    "            m_hat_hf = cell.m_W_hf / (1 - beta1 ** self.t)\n",
    "            v_hat_hf = cell.v_W_hf / (1 - beta2 ** self.t)\n",
    "            cell.W_hf -= lr * m_hat_hf / (np.sqrt(v_hat_hf) + eps)\n",
    "\n",
    "            cell.m_b_f = beta1 * cell.m_b_f + (1 - beta1) * cell.db_f\n",
    "            cell.v_b_f = beta2 * cell.v_b_f + (1 - beta2) * (cell.db_f ** 2)\n",
    "            m_hat_bf = cell.m_b_f / (1 - beta1 ** self.t)\n",
    "            v_hat_bf = cell.v_b_f / (1 - beta2 ** self.t)\n",
    "            cell.b_f -= lr * m_hat_bf / (np.sqrt(v_hat_bf) + eps)\n",
    "\n",
    "            # Update input gate weights\n",
    "            cell.m_W_ii = beta1 * cell.m_W_ii + (1 - beta1) * cell.dW_ii\n",
    "            cell.v_W_ii = beta2 * cell.v_W_ii + (1 - beta2) * (cell.dW_ii ** 2)\n",
    "            m_hat_ii = cell.m_W_ii / (1 - beta1 ** self.t)\n",
    "            v_hat_ii = cell.v_W_ii / (1 - beta2 ** self.t)\n",
    "            cell.W_ii -= lr * m_hat_ii / (np.sqrt(v_hat_ii) + eps)\n",
    "\n",
    "            cell.m_W_hi = beta1 * cell.m_W_hi + (1 - beta1) * cell.dW_hi\n",
    "            cell.v_W_hi = beta2 * cell.v_W_hi + (1 - beta2) * (cell.dW_hi ** 2)\n",
    "            m_hat_hi = cell.m_W_hi / (1 - beta1 ** self.t)\n",
    "            v_hat_hi = cell.v_W_hi / (1 - beta2 ** self.t)\n",
    "            cell.W_hi -= lr * m_hat_hi / (np.sqrt(v_hat_hi) + eps)\n",
    "\n",
    "            cell.m_b_i = beta1 * cell.m_b_i + (1 - beta1) * cell.db_i\n",
    "            cell.v_b_i = beta2 * cell.v_b_i + (1 - beta2) * (cell.db_i ** 2)\n",
    "            m_hat_bi = cell.m_b_i / (1 - beta1 ** self.t)\n",
    "            v_hat_bi = cell.v_b_i / (1 - beta2 ** self.t)\n",
    "            cell.b_i -= lr * m_hat_bi / (np.sqrt(v_hat_bi) + eps)\n",
    "\n",
    "            # Update cell gate weights\n",
    "            cell.m_W_ig = beta1 * cell.m_W_ig + (1 - beta1) * cell.dW_ig\n",
    "            cell.v_W_ig = beta2 * cell.v_W_ig + (1 - beta2) * (cell.dW_ig ** 2)\n",
    "            m_hat_ig = cell.m_W_ig / (1 - beta1 ** self.t)\n",
    "            v_hat_ig = cell.v_W_ig / (1 - beta2 ** self.t)\n",
    "            cell.W_ig -= lr * m_hat_ig / (np.sqrt(v_hat_ig) + eps)\n",
    "\n",
    "            cell.m_W_hg = beta1 * cell.m_W_hg + (1 - beta1) * cell.dW_hg\n",
    "            cell.v_W_hg = beta2 * cell.v_W_hg + (1 - beta2) * (cell.dW_hg ** 2)\n",
    "            m_hat_hg = cell.m_W_hg / (1 - beta1 ** self.t)\n",
    "            v_hat_hg = cell.v_W_hg / (1 - beta2 ** self.t)\n",
    "            cell.W_hg -= lr * m_hat_hg / (np.sqrt(v_hat_hg) + eps)\n",
    "\n",
    "            cell.m_b_g = beta1 * cell.m_b_g + (1 - beta1) * cell.db_g\n",
    "            cell.v_b_g = beta2 * cell.v_b_g + (1 - beta2) * (cell.db_g ** 2)\n",
    "            m_hat_bg = cell.m_b_g / (1 - beta1 ** self.t)\n",
    "            v_hat_bg = cell.v_b_g / (1 - beta2 ** self.t)\n",
    "            cell.b_g -= lr * m_hat_bg / (np.sqrt(v_hat_bg) + eps)\n",
    "\n",
    "            # Update output gate weights\n",
    "            cell.m_W_io = beta1 * cell.m_W_io + (1 - beta1) * cell.dW_io\n",
    "            cell.v_W_io = beta2 * cell.v_W_io + (1 - beta2) * (cell.dW_io ** 2)\n",
    "            m_hat_io = cell.m_W_io / (1 - beta1 ** self.t)\n",
    "            v_hat_io = cell.v_W_io / (1 - beta2 ** self.t)\n",
    "            cell.W_io -= lr * m_hat_io / (np.sqrt(v_hat_io) + eps)\n",
    "\n",
    "            cell.m_W_ho = beta1 * cell.m_W_ho + (1 - beta1) * cell.dW_ho\n",
    "            cell.v_W_ho = beta2 * cell.v_W_ho + (1 - beta2) * (cell.dW_ho ** 2)\n",
    "            m_hat_ho = cell.m_W_ho / (1 - beta1 ** self.t)\n",
    "            v_hat_ho = cell.v_W_ho / (1 - beta2 ** self.t)\n",
    "            cell.W_ho -= lr * m_hat_ho / (np.sqrt(v_hat_ho) + eps)\n",
    "\n",
    "            cell.m_b_o = beta1 * cell.m_b_o + (1 - beta1) * cell.db_o\n",
    "            cell.v_b_o = beta2 * cell.v_b_o + (1 - beta2) * (cell.db_o ** 2)\n",
    "            m_hat_bo = cell.m_b_o / (1 - beta1 ** self.t)\n",
    "            v_hat_bo = cell.v_b_o / (1 - beta2 ** self.t)\n",
    "            cell.b_o -= lr * m_hat_bo / (np.sqrt(v_hat_bo) + eps)\n",
    "\n",
    "    def predict(self, input_ids, intent_ids):\n",
    "        logits = self.forward(input_ids, intent_ids, training=False)\n",
    "        return np.argmax(logits, axis=2)\n",
    "# ============================================================================\n",
    "# INTENT CLASSIFIER MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class IntentClassifier:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_intents):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_intents = num_intents\n",
    "\n",
    "        self.embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "        self.encoder = BiRNN(embed_dim, hidden_dim)\n",
    "\n",
    "        self.W_out = np.random.randn(2 * hidden_dim, num_intents) * 0.01\n",
    "        self.b_out = np.zeros((1, num_intents))\n",
    "\n",
    "        self.m_W_out = np.zeros_like(self.W_out)\n",
    "        self.v_W_out = np.zeros_like(self.W_out)\n",
    "        self.m_b_out = np.zeros_like(self.b_out)\n",
    "        self.v_b_out = np.zeros_like(self.b_out)\n",
    "        self.m_emb = np.zeros_like(self.embeddings)\n",
    "        self.v_emb = np.zeros_like(self.embeddings)\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, input_ids, training=True):\n",
    "        self.input_ids = input_ids\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        self.embeds = self.embeddings[input_ids]\n",
    "\n",
    "        if training:\n",
    "            self.dropout_mask = (np.random.rand(*self.embeds.shape) > 0.3).astype(float) / 0.7\n",
    "            embeds_dropped = self.embeds * self.dropout_mask\n",
    "        else:\n",
    "            embeds_dropped = self.embeds\n",
    "\n",
    "        self.rnn_outputs = self.encoder.forward(embeds_dropped)\n",
    "        self.final_hidden = self.rnn_outputs[:, -1, :]\n",
    "        self.logits = self.final_hidden @ self.W_out + self.b_out\n",
    "\n",
    "        return self.logits\n",
    "\n",
    "    def backward(self, dloss):\n",
    "        batch_size = dloss.shape[0]\n",
    "\n",
    "        dW_out = self.final_hidden.T @ dloss\n",
    "        db_out = np.sum(dloss, axis=0, keepdims=True)\n",
    "\n",
    "        dfinal_hidden = dloss @ self.W_out.T\n",
    "\n",
    "        drnn_outputs = np.zeros_like(self.rnn_outputs)\n",
    "        drnn_outputs[:, -1, :] = dfinal_hidden\n",
    "\n",
    "        self.encoder.backward(drnn_outputs)\n",
    "\n",
    "        return dW_out, db_out\n",
    "\n",
    "    def update(self, dW_out, db_out, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.t += 1\n",
    "\n",
    "        self.m_W_out = beta1 * self.m_W_out + (1 - beta1) * dW_out\n",
    "        self.v_W_out = beta2 * self.v_W_out + (1 - beta2) * (dW_out ** 2)\n",
    "        m_hat = self.m_W_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_W_out / (1 - beta2 ** self.t)\n",
    "        self.W_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        self.m_b_out = beta1 * self.m_b_out + (1 - beta1) * db_out\n",
    "        self.v_b_out = beta2 * self.v_b_out + (1 - beta2) * (db_out ** 2)\n",
    "        m_hat = self.m_b_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_b_out / (1 - beta2 ** self.t)\n",
    "        self.b_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        for cell in self.encoder.forward_cells + self.encoder.backward_cells:\n",
    "            m_W_ih = beta1 * cell.m_W_ih + (1 - beta1) * cell.dW_ih\n",
    "            v_W_ih = beta2 * cell.v_W_ih + (1 - beta2) * (cell.dW_ih ** 2)\n",
    "            m_hat = m_W_ih / (1 - beta1 ** self.t)\n",
    "            v_hat = v_W_ih / (1 - beta2 ** self.t)\n",
    "            cell.W_ih -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_W_ih = m_W_ih\n",
    "            cell.v_W_ih = v_W_ih\n",
    "\n",
    "            m_W_hh = beta1 * cell.m_W_hh + (1 - beta1) * cell.dW_hh\n",
    "            v_W_hh = beta2 * cell.v_W_hh + (1 - beta2) * (cell.dW_hh ** 2)\n",
    "            m_hat = m_W_hh / (1 - beta1 ** self.t)\n",
    "            v_hat = v_W_hh / (1 - beta2 ** self.t)\n",
    "            cell.W_hh -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_W_hh = m_W_hh\n",
    "            cell.v_W_hh = v_W_hh\n",
    "\n",
    "            m_b_h = beta1 * cell.m_b_h + (1 - beta1) * cell.db_h\n",
    "            v_b_h = beta2 * cell.v_b_h + (1 - beta2) * (cell.db_h ** 2)\n",
    "            m_hat = m_b_h / (1 - beta1 ** self.t)\n",
    "            v_hat = v_b_h / (1 - beta2 ** self.t)\n",
    "            cell.b_h -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_b_h = m_b_h\n",
    "            cell.v_b_h = v_b_h\n",
    "\n",
    "    def predict(self, input_ids):\n",
    "        logits = self.forward(input_ids, training=False)\n",
    "        return np.argmax(logits, axis=1)\n",
    "\n",
    "# ============================================================================\n",
    "# SLOT FILLING MODEL WITH INTENT\n",
    "# ============================================================================\n",
    "\n",
    "class SlotFillingWithIntent:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_slots, num_intents):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_slots = num_slots\n",
    "        self.num_intents = num_intents\n",
    "\n",
    "        self.word_embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "        self.intent_embeddings = np.random.randn(num_intents, 32) * 0.01\n",
    "\n",
    "        self.encoder = BiRNN(embed_dim + 32, hidden_dim)\n",
    "\n",
    "        self.W_out = np.random.randn(2 * hidden_dim, num_slots) * 0.01\n",
    "        self.b_out = np.zeros((1, num_slots))\n",
    "\n",
    "        self.m_W_out = np.zeros_like(self.W_out)\n",
    "        self.v_W_out = np.zeros_like(self.W_out)\n",
    "        self.m_b_out = np.zeros_like(self.b_out)\n",
    "        self.v_b_out = np.zeros_like(self.b_out)\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, input_ids, intent_ids, training=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.intent_ids = intent_ids\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        self.word_embeds = self.word_embeddings[input_ids]\n",
    "\n",
    "        intent_embeds = self.intent_embeddings[intent_ids]\n",
    "        self.intent_embeds = np.repeat(intent_embeds[:, np.newaxis, :], seq_len, axis=1)\n",
    "\n",
    "        self.combined = np.concatenate([self.word_embeds, self.intent_embeds], axis=2)\n",
    "\n",
    "        if training:\n",
    "            self.dropout_mask = (np.random.rand(*self.combined.shape) > 0.3).astype(float) / 0.7\n",
    "            combined_dropped = self.combined * self.dropout_mask\n",
    "        else:\n",
    "            combined_dropped = self.combined\n",
    "\n",
    "        self.rnn_outputs = self.encoder.forward(combined_dropped)\n",
    "\n",
    "        batch_size, seq_len, hidden_size = self.rnn_outputs.shape\n",
    "        rnn_flat = self.rnn_outputs.reshape(-1, hidden_size)\n",
    "        logits_flat = rnn_flat @ self.W_out + self.b_out\n",
    "        self.logits = logits_flat.reshape(batch_size, seq_len, self.num_slots)\n",
    "\n",
    "        return self.logits\n",
    "\n",
    "    def backward(self, dloss):\n",
    "        batch_size, seq_len, _ = dloss.shape\n",
    "        hidden_size = 2 * self.hidden_dim\n",
    "\n",
    "        dloss_flat = dloss.reshape(-1, self.num_slots)\n",
    "        rnn_flat = self.rnn_outputs.reshape(-1, hidden_size)\n",
    "\n",
    "        dW_out = rnn_flat.T @ dloss_flat\n",
    "        db_out = np.sum(dloss_flat, axis=0, keepdims=True)\n",
    "\n",
    "        drnn_flat = dloss_flat @ self.W_out.T\n",
    "        drnn_outputs = drnn_flat.reshape(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        self.encoder.backward(drnn_outputs)\n",
    "\n",
    "        return dW_out, db_out\n",
    "\n",
    "    def update(self, dW_out, db_out, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.t += 1\n",
    "\n",
    "        self.m_W_out = beta1 * self.m_W_out + (1 - beta1) * dW_out\n",
    "        self.v_W_out = beta2 * self.v_W_out + (1 - beta2) * (dW_out ** 2)\n",
    "        m_hat = self.m_W_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_W_out / (1 - beta2 ** self.t)\n",
    "        self.W_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        self.m_b_out = beta1 * self.m_b_out + (1 - beta1) * db_out\n",
    "        self.v_b_out = beta2 * self.v_b_out + (1 - beta2) * (db_out ** 2)\n",
    "        m_hat = self.m_b_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_b_out / (1 - beta2 ** self.t)\n",
    "        self.b_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        for cell in self.encoder.forward_cells + self.encoder.backward_cells:\n",
    "            m_W_ih = beta1 * cell.m_W_ih + (1 - beta1) * cell.dW_ih\n",
    "            v_W_ih = beta2 * cell.v_W_ih + (1 - beta2) * (cell.dW_ih ** 2)\n",
    "            m_hat = m_W_ih / (1 - beta1 ** self.t)\n",
    "            v_hat = v_W_ih / (1 - beta2 ** self.t)\n",
    "            cell.W_ih -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_W_ih = m_W_ih\n",
    "            cell.v_W_ih = v_W_ih\n",
    "\n",
    "            m_W_hh = beta1 * cell.m_W_hh + (1 - beta1) * cell.dW_hh\n",
    "            v_W_hh = beta2 * cell.v_W_hh + (1 - beta2) * (cell.dW_hh ** 2)\n",
    "            m_hat = m_W_hh / (1 - beta1 ** self.t)\n",
    "            v_hat = v_W_hh / (1 - beta2 ** self.t)\n",
    "            cell.W_hh -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_W_hh = m_W_hh\n",
    "            cell.v_W_hh = v_W_hh\n",
    "\n",
    "            m_b_h = beta1 * cell.m_b_h + (1 - beta1) * cell.db_h\n",
    "            v_b_h = beta2 * cell.v_b_h + (1 - beta2) * (cell.db_h ** 2)\n",
    "            m_hat = m_b_h / (1 - beta1 ** self.t)\n",
    "            v_hat = v_b_h / (1 - beta2 ** self.t)\n",
    "            cell.b_h -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_b_h = m_b_h\n",
    "            cell.v_b_h = v_b_h\n",
    "\n",
    "    def predict(self, input_ids, intent_ids):\n",
    "        logits = self.forward(input_ids, intent_ids, training=False)\n",
    "        return np.argmax(logits, axis=2)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def train_intent_classifier(model, train_data, val_data, epochs=15, batch_size=32, lr=0.001):\n",
    "    print(\"Training Intent Classifier...\")\n",
    "    \n",
    "    if len(train_data) == 0:\n",
    "        print(\"ERROR: No training data available!\")\n",
    "        return model\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(train_data)\n",
    "\n",
    "        total_loss = 0\n",
    "        num_batches = max(1, len(train_data) // batch_size)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Create progress bar\n",
    "        pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{epochs}\", ncols=100)\n",
    "        \n",
    "        for i in pbar:\n",
    "            batch = train_data[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "\n",
    "            max_len = max(len(item['input_ids']) for item in batch)\n",
    "            input_ids = np.zeros((len(batch), max_len), dtype=int)\n",
    "            intents = np.zeros(len(batch), dtype=int)\n",
    "\n",
    "            for j, item in enumerate(batch):\n",
    "                length = len(item['input_ids'])\n",
    "                input_ids[j, :length] = item['input_ids']\n",
    "                intents[j] = item['intent']\n",
    "\n",
    "            logits = model.forward(input_ids, training=True)\n",
    "\n",
    "            probs = softmax(logits)\n",
    "            loss = -np.mean(np.log(probs[range(len(batch)), intents] + 1e-10))\n",
    "            total_loss += loss\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            preds = np.argmax(logits, axis=1)\n",
    "            correct += np.sum(preds == intents)\n",
    "            total += len(batch)\n",
    "\n",
    "            dloss = probs.copy()\n",
    "            dloss[range(len(batch)), intents] -= 1\n",
    "            dloss /= len(batch)\n",
    "\n",
    "            dW_out, db_out = model.backward(dloss)\n",
    "            model.update(dW_out, db_out, lr=lr)\n",
    "            \n",
    "            # Update progress bar with current loss and accuracy\n",
    "            train_acc = correct / total if total > 0 else 0\n",
    "            pbar.set_postfix({'loss': f'{loss:.4f}', 'acc': f'{train_acc:.4f}'})\n",
    "\n",
    "        train_acc = correct / total if total > 0 else 0\n",
    "        val_acc = evaluate_intent(model, val_data, batch_size) if len(val_data) > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/num_batches:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(f\"  ✓ New best validation accuracy: {val_acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "def train_slot_filler(slot_model, intent_model, train_data, val_data, epochs=15, batch_size=32, lr=0.001):\n",
    "    print(\"Training Slot Filler...\")\n",
    "    \n",
    "    if len(train_data) == 0:\n",
    "        print(\"ERROR: No training data available!\")\n",
    "        return slot_model\n",
    "\n",
    "    best_f1 = 0\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(train_data)\n",
    "\n",
    "        total_loss = 0\n",
    "        num_batches = max(1, len(train_data) // batch_size)\n",
    "\n",
    "        # Create progress bar\n",
    "        pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{epochs}\", ncols=100)\n",
    "        \n",
    "        for i in pbar:\n",
    "            batch = train_data[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "\n",
    "            max_len = max(len(item['input_ids']) for item in batch)\n",
    "            input_ids = np.zeros((len(batch), max_len), dtype=int)\n",
    "            slots = np.full((len(batch), max_len), -100, dtype=int)\n",
    "\n",
    "            for j, item in enumerate(batch):\n",
    "                length = len(item['input_ids'])\n",
    "                input_ids[j, :length] = item['input_ids']\n",
    "                slots[j, :length] = item['slots']\n",
    "\n",
    "            predicted_intents = intent_model.predict(input_ids)\n",
    "\n",
    "            logits = slot_model.forward(input_ids, predicted_intents, training=True)\n",
    "\n",
    "            probs = softmax(logits)\n",
    "            loss = 0\n",
    "            count = 0\n",
    "\n",
    "            for b in range(len(batch)):\n",
    "                for t in range(max_len):\n",
    "                    if slots[b, t] != -100:\n",
    "                        loss += -np.log(probs[b, t, slots[b, t]] + 1e-10)\n",
    "                        count += 1\n",
    "\n",
    "            if count > 0:\n",
    "                loss /= count\n",
    "                total_loss += loss\n",
    "\n",
    "                dloss = probs.copy()\n",
    "                for b in range(len(batch)):\n",
    "                    for t in range(max_len):\n",
    "                        if slots[b, t] != -100:\n",
    "                            dloss[b, t, slots[b, t]] -= 1\n",
    "                dloss /= count\n",
    "\n",
    "                dW_out, db_out = slot_model.backward(dloss)\n",
    "                slot_model.update(dW_out, db_out, lr=lr)\n",
    "                \n",
    "                # Update progress bar with current loss\n",
    "                pbar.set_postfix({'loss': f'{loss:.4f}'})\n",
    "\n",
    "        metrics = evaluate_slot(slot_model, intent_model, val_data, batch_size) if len(val_data) > 0 else {'f1': 0, 'accuracy': 0}\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/num_batches:.4f}, Val F1: {metrics['f1']:.4f}, Val Acc: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            print(f\"  ✓ New best validation F1: {metrics['f1']:.4f}\")\n",
    "    return slot_model\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_intent(model, data, batch_size=32):\n",
    "    \"\"\"Evaluate intent classifier\"\"\"\n",
    "    if len(data) == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    num_batches = max(1, len(data) // batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = data[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        if len(batch) == 0:\n",
    "            continue\n",
    "\n",
    "        max_len = max(len(item['input_ids']) for item in batch)\n",
    "        input_ids = np.zeros((len(batch), max_len), dtype=int)\n",
    "        intents = np.zeros(len(batch), dtype=int)\n",
    "\n",
    "        for j, item in enumerate(batch):\n",
    "            length = len(item['input_ids'])\n",
    "            input_ids[j, :length] = item['input_ids']\n",
    "            intents[j] = item['intent']\n",
    "\n",
    "        intent_preds = model.predict(input_ids)\n",
    "\n",
    "        all_preds.extend(intent_preds)\n",
    "        all_labels.extend(intents)\n",
    "\n",
    "    if len(all_preds) == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate_slot(slot_model, intent_model, data, batch_size=32):\n",
    "    \"\"\"Evaluate slot filling model\"\"\"\n",
    "    if len(data) == 0:\n",
    "        return {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "        \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    num_batches = max(1, len(data) // batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = data[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        if len(batch) == 0:\n",
    "            continue\n",
    "\n",
    "        max_len = max(len(item['input_ids']) for item in batch)\n",
    "        input_ids = np.zeros((len(batch), max_len), dtype=int)\n",
    "        slots = np.full((len(batch), max_len), -100, dtype=int)\n",
    "        lengths = []\n",
    "\n",
    "        for j, item in enumerate(batch):\n",
    "            length = len(item['input_ids'])\n",
    "            input_ids[j, :length] = item['input_ids']\n",
    "            slots[j, :length] = item['slots']\n",
    "            lengths.append(length)\n",
    "\n",
    "        predicted_intents = intent_model.predict(input_ids)\n",
    "        slot_preds = slot_model.predict(input_ids, predicted_intents)\n",
    "\n",
    "        for j in range(len(batch)):\n",
    "            all_preds.extend(slot_preds[j, :lengths[j]])\n",
    "            all_labels.extend(slots[j, :lengths[j]])\n",
    "\n",
    "    if len(all_preds) == 0:\n",
    "        return {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "\n",
    "def main(train_file=None, val_file=None, test_file=None, dataset_type='slurp', model_type='rnn'):\n",
    "    \"\"\"Main function to run the entire pipeline\"\"\"\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    # Check if running in notebook or command line\n",
    "    if train_file is None:\n",
    "        if len(sys.argv) < 4:\n",
    "            print(\"⚠️  No file paths provided. Please call main() with file paths:\")\n",
    "            print(\"Example: main('train.jsonl', 'val.jsonl', 'test.jsonl', 'slurp')\")\n",
    "            return None  # Changed from sys.exit(1)\n",
    "        \n",
    "        train_file = sys.argv[1]\n",
    "        val_file = sys.argv[2]\n",
    "        test_file = sys.argv[3]\n",
    "        dataset_type = sys.argv[4] if len(sys.argv) > 4 else 'slurp'\n",
    "    \n",
    "    # Validate files exist\n",
    "    for file_path in [train_file, val_file, test_file]:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File not found - {file_path}\")\n",
    "            print(f\"Current directory: {os.getcwd()}\")\n",
    "            print(f\"Files available: {[f for f in os.listdir('.') if f.endswith('.jsonl')]}\")\n",
    "            return None  # Changed from sys.exit(1)\n",
    "    \n",
    "    print(f\"Dataset type: {dataset_type}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load datasets\n",
    "    train_data = load_json_dataset(train_file, dataset_type)\n",
    "    val_data = load_json_dataset(val_file, dataset_type)\n",
    "    test_data = load_json_dataset(test_file, dataset_type)\n",
    "    \n",
    "    # Check if we have any data\n",
    "    if len(train_data) == 0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CRITICAL ERROR: No training data loaded!\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\nPlease verify your JSONL file format. Here's an example:\")\n",
    "        if dataset_type == 'slurp':\n",
    "            print('\\nFor SLURP format, each line should be:')\n",
    "            print('{\"tokens\": [\"play\", \"some\", \"music\"], \"scenario\": \"music\", \"action\": \"play\", \"entities\": []}')\n",
    "        else:\n",
    "            print('\\nFor ATIS format, each line should be:')\n",
    "            print('{\"tokens\": [\"show\", \"flights\"], \"intent\": \"flight\", \"slots\": [\"O\", \"O\"]}')\n",
    "        return None  # Changed from sys.exit(1)\n",
    "    \n",
    "    # Build vocabulary and mappings\n",
    "    vocab, intent2idx, slot2idx = build_vocab_and_mappings(train_data, val_data)\n",
    "    \n",
    "    if len(intent2idx) == 0:\n",
    "        print(\"\\nERROR: No intents found in data!\")\n",
    "        return None  # Changed from sys.exit(1)\n",
    "    \n",
    "    # Convert to IDs\n",
    "    train_processed = convert_to_ids(train_data, vocab, intent2idx, slot2idx)\n",
    "    val_processed = convert_to_ids(val_data, vocab, intent2idx, slot2idx)\n",
    "    test_processed = convert_to_ids(test_data, vocab, intent2idx, slot2idx)\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    EMBED_DIM = 256  # Increased from 128\n",
    "    HIDDEN_DIM = 256  # Increased from 128\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 25  # Increased from 15\n",
    "    LR = 0.0005  # Decreased from 0.001 for more stable training\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PHASE 1: Training Intent Classifier ({model_type.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize intent classifier based on model type\n",
    "    if model_type == 'lstm':\n",
    "        intent_model = IntentClassifierLSTM(\n",
    "            vocab_size=len(vocab),\n",
    "            embed_dim=EMBED_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_intents=len(intent2idx)\n",
    "        )\n",
    "    else:  # 'rnn'\n",
    "        intent_model = IntentClassifier(\n",
    "            vocab_size=len(vocab),\n",
    "            embed_dim=EMBED_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_intents=len(intent2idx)\n",
    "        )\n",
    "    \n",
    "    intent_model = train_intent_classifier(\n",
    "        intent_model, \n",
    "        train_processed, \n",
    "        val_processed,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR\n",
    "    )\n",
    "    \n",
    "    # Evaluate intent classifier on test set\n",
    "    print(\"\\nEvaluating Intent Classifier on Test Set...\")\n",
    "    test_intent_acc = evaluate_intent(intent_model, test_processed, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test Intent Accuracy: {test_intent_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PHASE 2: Training Slot Filler ({model_type.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize slot filler based on model type\n",
    "    if model_type == 'lstm':\n",
    "        slot_model = SlotFillingWithIntentLSTM(\n",
    "            vocab_size=len(vocab),\n",
    "            embed_dim=EMBED_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_slots=len(slot2idx),\n",
    "            num_intents=len(intent2idx)\n",
    "        )\n",
    "    else:  # 'rnn'\n",
    "        slot_model = SlotFillingWithIntent(\n",
    "            vocab_size=len(vocab),\n",
    "            embed_dim=EMBED_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_slots=len(slot2idx),\n",
    "            num_intents=len(intent2idx)\n",
    "        )\n",
    "    \n",
    "    slot_model = train_slot_filler(\n",
    "        slot_model,\n",
    "        intent_model,\n",
    "        train_processed,\n",
    "        val_processed,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR\n",
    "    )\n",
    "    \n",
    "    # Evaluate slot filler on test set\n",
    "    print(\"\\nEvaluating Slot Filler on Test Set...\")\n",
    "    test_metrics = evaluate_slot(slot_model, intent_model, test_processed, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test Slot Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Test Slot Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"Test Slot Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"Test Slot F1: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Intent Classification - Test Accuracy: {test_intent_acc:.4f}\")\n",
    "    print(f\"Slot Filling - Test F1: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"Slot Filling - Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'intent_model': intent_model,\n",
    "        'slot_model': slot_model,\n",
    "        'vocab': vocab,\n",
    "        'intent2idx': intent2idx,\n",
    "        'slot2idx': slot2idx,\n",
    "        'test_intent_acc': test_intent_acc,\n",
    "        'test_slot_metrics': test_metrics\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_ipython()\n",
    "        IN_NOTEBOOK = True\n",
    "    except NameError:\n",
    "        IN_NOTEBOOK = False\n",
    "    \n",
    "    if IN_NOTEBOOK:\n",
    "        print(\"Running in Jupyter Notebook mode\")\n",
    "        results = main(\n",
    "            train_file='../cleaned-datasets/slurp/train.jsonl',\n",
    "            val_file='../cleaned-datasets/slurp/devel.jsonl',\n",
    "            test_file='../cleaned-datasets/slurp/test.jsonl',\n",
    "            dataset_type='slurp',\n",
    "            model_type='rnn'  # <--- ADD THIS LINE! Change to 'rnn' to use RNN\n",
    "        )\n",
    "    # else:\n",
    "    #     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
