{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d1d3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter Notebook mode\n",
      "Dataset type: atis\n",
      "================================================================================\n",
      "Loading ATIS CSV datasets...\n",
      "Loading ATIS CSV from ../cleaned-datasets/atis/train.csv...\n",
      "CSV columns found: ['intent', 'text', 'slots']\n",
      "Using 'text' column for tokens\n",
      "Loaded 4501 samples from CSV\n",
      "\n",
      "Loading ATIS CSV from ../cleaned-datasets/atis/test.csv...\n",
      "CSV columns found: ['intent', 'text', 'slots']\n",
      "Using 'text' column for tokens\n",
      "Loaded 754 samples from CSV\n",
      "\n",
      "\n",
      "Splitting training data into train/val (80/20)...\n",
      "Train: 3600, Val: 901, Test: 754\n",
      "\n",
      "Building vocabulary and mappings...\n",
      "Vocabulary size: 799\n",
      "Number of intents: 4\n",
      "Number of slots: 117\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: Training Intent Classifier (LSTM)\n",
      "================================================================================\n",
      "Training Intent Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|████████████████████████| 112/112 [00:25<00:00,  4.44it/s, loss=0.9090, acc=0.8122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Loss: 0.8470, Train Acc: 0.8122, Val Acc: 0.7812\n",
      "  ✓ New best validation accuracy: 0.7812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|████████████████████████| 112/112 [00:29<00:00,  3.83it/s, loss=0.5746, acc=0.8334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Loss: 0.4884, Train Acc: 0.8334, Val Acc: 0.8270\n",
      "  ✓ New best validation accuracy: 0.8270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|████████████████████████| 112/112 [00:24<00:00,  4.55it/s, loss=1.2304, acc=0.8507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Loss: 0.4039, Train Acc: 0.8507, Val Acc: 0.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|████████████████████████| 112/112 [00:25<00:00,  4.42it/s, loss=0.2998, acc=0.8301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Loss: 0.5277, Train Acc: 0.8301, Val Acc: 0.8203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|████████████████████████| 112/112 [00:27<00:00,  4.01it/s, loss=0.5193, acc=0.7983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Loss: 0.5049, Train Acc: 0.7983, Val Acc: 0.8058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|████████████████████████| 112/112 [00:30<00:00,  3.70it/s, loss=0.2323, acc=0.8195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Loss: 0.4185, Train Acc: 0.8195, Val Acc: 0.8382\n",
      "  ✓ New best validation accuracy: 0.8382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|████████████████████████| 112/112 [00:23<00:00,  4.78it/s, loss=0.2105, acc=0.8569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Loss: 0.3758, Train Acc: 0.8569, Val Acc: 0.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|████████████████████████| 112/112 [00:23<00:00,  4.85it/s, loss=0.4418, acc=0.8814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Loss: 0.3484, Train Acc: 0.8814, Val Acc: 0.8583\n",
      "  ✓ New best validation accuracy: 0.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|████████████████████████| 112/112 [00:23<00:00,  4.81it/s, loss=0.3340, acc=0.8940]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Loss: 0.3254, Train Acc: 0.8940, Val Acc: 0.8337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|███████████████████████| 112/112 [00:23<00:00,  4.78it/s, loss=0.0904, acc=0.8836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Loss: 0.3075, Train Acc: 0.8836, Val Acc: 0.8806\n",
      "  ✓ New best validation accuracy: 0.8806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|███████████████████████| 112/112 [00:24<00:00,  4.63it/s, loss=0.3106, acc=0.8990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Loss: 0.2897, Train Acc: 0.8990, Val Acc: 0.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|███████████████████████| 112/112 [00:24<00:00,  4.66it/s, loss=0.1648, acc=0.8987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Loss: 0.2877, Train Acc: 0.8987, Val Acc: 0.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|███████████████████████| 112/112 [00:25<00:00,  4.34it/s, loss=0.4061, acc=0.8767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Loss: 0.3292, Train Acc: 0.8767, Val Acc: 0.8516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|███████████████████████| 112/112 [00:29<00:00,  3.74it/s, loss=0.3041, acc=0.8867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Loss: 0.2979, Train Acc: 0.8867, Val Acc: 0.8616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|███████████████████████| 112/112 [00:27<00:00,  4.12it/s, loss=0.2509, acc=0.9040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Loss: 0.2690, Train Acc: 0.9040, Val Acc: 0.8717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|███████████████████████| 112/112 [00:24<00:00,  4.48it/s, loss=0.3398, acc=0.8756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Loss: 0.3617, Train Acc: 0.8756, Val Acc: 0.8415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|███████████████████████| 112/112 [00:26<00:00,  4.29it/s, loss=0.2581, acc=0.8836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Loss: 0.3007, Train Acc: 0.8836, Val Acc: 0.8605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|███████████████████████| 112/112 [00:25<00:00,  4.33it/s, loss=0.3733, acc=0.8747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Loss: 0.3071, Train Acc: 0.8747, Val Acc: 0.8348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|███████████████████████| 112/112 [00:26<00:00,  4.26it/s, loss=0.2567, acc=0.9065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - Loss: 0.2650, Train Acc: 0.9065, Val Acc: 0.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|███████████████████████| 112/112 [00:32<00:00,  3.40it/s, loss=0.3168, acc=0.9174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - Loss: 0.2510, Train Acc: 0.9174, Val Acc: 0.8638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|███████████████████████| 112/112 [00:25<00:00,  4.32it/s, loss=0.2179, acc=0.9155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - Loss: 0.2447, Train Acc: 0.9155, Val Acc: 0.8862\n",
      "  ✓ New best validation accuracy: 0.8862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|███████████████████████| 112/112 [00:24<00:00,  4.63it/s, loss=0.1646, acc=0.9118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - Loss: 0.2521, Train Acc: 0.9118, Val Acc: 0.8817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|███████████████████████| 112/112 [00:25<00:00,  4.33it/s, loss=0.3399, acc=0.9099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - Loss: 0.2460, Train Acc: 0.9099, Val Acc: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|███████████████████████| 112/112 [00:27<00:00,  4.12it/s, loss=0.4563, acc=0.9169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - Loss: 0.2370, Train Acc: 0.9169, Val Acc: 0.8806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|███████████████████████| 112/112 [00:27<00:00,  4.02it/s, loss=0.2841, acc=0.9233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - Loss: 0.2462, Train Acc: 0.9233, Val Acc: 0.8917\n",
      "  ✓ New best validation accuracy: 0.8917\n",
      "\n",
      "Evaluating Intent Classifier on Test Set...\n",
      "Test Intent Accuracy: 0.9158\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: Training Slot Filler (LSTM)\n",
      "================================================================================\n",
      "Training Slot Filler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|████████████████████████████████████| 112/112 [00:35<00:00,  3.13it/s, loss=1.5193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Loss: 2.3385, Val F1: 0.4979, Val Acc: 0.6387\n",
      "  ✓ New best validation F1: 0.4979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|████████████████████████████████████| 112/112 [00:41<00:00,  2.67it/s, loss=1.6744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Loss: 1.6608, Val F1: 0.4979, Val Acc: 0.6387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|████████████████████████████████████| 112/112 [00:30<00:00,  3.63it/s, loss=1.8196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Loss: 1.6663, Val F1: 0.4979, Val Acc: 0.6387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|████████████████████████████████████| 112/112 [00:32<00:00,  3.50it/s, loss=1.6073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Loss: 1.6668, Val F1: 0.4979, Val Acc: 0.6387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|████████████████████████████████████| 112/112 [00:30<00:00,  3.62it/s, loss=1.7271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Loss: 1.6639, Val F1: 0.4981, Val Acc: 0.6387\n",
      "  ✓ New best validation F1: 0.4981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|████████████████████████████████████| 112/112 [00:32<00:00,  3.50it/s, loss=1.4092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Loss: 1.6619, Val F1: 0.4979, Val Acc: 0.6387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|████████████████████████████████████| 112/112 [00:28<00:00,  3.87it/s, loss=1.6198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Loss: 1.6519, Val F1: 0.5010, Val Acc: 0.6400\n",
      "  ✓ New best validation F1: 0.5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|████████████████████████████████████| 112/112 [00:29<00:00,  3.85it/s, loss=1.5174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Loss: 1.6412, Val F1: 0.5018, Val Acc: 0.6406\n",
      "  ✓ New best validation F1: 0.5018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|████████████████████████████████████| 112/112 [00:32<00:00,  3.42it/s, loss=1.7434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Loss: 1.6337, Val F1: 0.5019, Val Acc: 0.6407\n",
      "  ✓ New best validation F1: 0.5019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|███████████████████████████████████| 112/112 [00:35<00:00,  3.14it/s, loss=1.6299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Loss: 1.6277, Val F1: 0.5024, Val Acc: 0.6412\n",
      "  ✓ New best validation F1: 0.5024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|███████████████████████████████████| 112/112 [00:32<00:00,  3.44it/s, loss=1.7438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Loss: 1.6226, Val F1: 0.5019, Val Acc: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|███████████████████████████████████| 112/112 [00:33<00:00,  3.32it/s, loss=1.6858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Loss: 1.6137, Val F1: 0.4982, Val Acc: 0.6387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|███████████████████████████████████| 112/112 [00:33<00:00,  3.38it/s, loss=1.4628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Loss: 1.6087, Val F1: 0.5061, Val Acc: 0.6433\n",
      "  ✓ New best validation F1: 0.5061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|███████████████████████████████████| 112/112 [00:30<00:00,  3.70it/s, loss=1.5835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Loss: 1.6052, Val F1: 0.5062, Val Acc: 0.6430\n",
      "  ✓ New best validation F1: 0.5062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|███████████████████████████████████| 112/112 [00:28<00:00,  3.87it/s, loss=1.4325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Loss: 1.5982, Val F1: 0.5032, Val Acc: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|███████████████████████████████████| 112/112 [00:29<00:00,  3.81it/s, loss=1.5746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Loss: 1.5953, Val F1: 0.4988, Val Acc: 0.6391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|███████████████████████████████████| 112/112 [00:30<00:00,  3.62it/s, loss=1.6276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Loss: 1.5926, Val F1: 0.5063, Val Acc: 0.6436\n",
      "  ✓ New best validation F1: 0.5063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|███████████████████████████████████| 112/112 [00:32<00:00,  3.47it/s, loss=1.5360]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Loss: 1.5884, Val F1: 0.5055, Val Acc: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|███████████████████████████████████| 112/112 [00:29<00:00,  3.80it/s, loss=1.6059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - Loss: 1.5806, Val F1: 0.5042, Val Acc: 0.6420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|███████████████████████████████████| 112/112 [00:30<00:00,  3.63it/s, loss=1.6934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - Loss: 1.5777, Val F1: 0.5065, Val Acc: 0.6435\n",
      "  ✓ New best validation F1: 0.5065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|███████████████████████████████████| 112/112 [00:30<00:00,  3.64it/s, loss=1.7386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - Loss: 1.5737, Val F1: 0.5083, Val Acc: 0.6440\n",
      "  ✓ New best validation F1: 0.5083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|███████████████████████████████████| 112/112 [00:30<00:00,  3.68it/s, loss=1.5090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - Loss: 1.5699, Val F1: 0.5085, Val Acc: 0.6447\n",
      "  ✓ New best validation F1: 0.5085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|███████████████████████████████████| 112/112 [00:29<00:00,  3.74it/s, loss=1.7036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - Loss: 1.5682, Val F1: 0.5083, Val Acc: 0.6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|███████████████████████████████████| 112/112 [00:30<00:00,  3.62it/s, loss=1.6226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - Loss: 1.5633, Val F1: 0.5103, Val Acc: 0.6456\n",
      "  ✓ New best validation F1: 0.5103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|███████████████████████████████████| 112/112 [00:32<00:00,  3.42it/s, loss=1.5656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - Loss: 1.5584, Val F1: 0.5105, Val Acc: 0.6460\n",
      "  ✓ New best validation F1: 0.5105\n",
      "\n",
      "Evaluating Slot Filler on Test Set...\n",
      "Test Slot Accuracy: 0.5867\n",
      "Test Slot Precision: 0.3579\n",
      "Test Slot Recall: 0.5867\n",
      "Test Slot F1: 0.4387\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "Intent Classification - Test Accuracy: 0.9158\n",
      "Slot Filling - Test F1: 0.4387\n",
      "Slot Filling - Test Accuracy: 0.5867\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def parse_atis_sample(sample):\n",
    "    \"\"\"Parse ATIS JSON sample\"\"\"\n",
    "    if 'tokens' in sample:\n",
    "        tokens = sample['tokens']\n",
    "    elif 'text' in sample:\n",
    "        tokens = sample['text'].split() if isinstance(sample['text'], str) else sample['text']\n",
    "    else:\n",
    "        tokens = []\n",
    "\n",
    "    intent = sample.get('intent', sample.get('label', sample.get('intent_label', '')))\n",
    "\n",
    "    if 'slots' in sample:\n",
    "        slots = sample['slots']\n",
    "    elif 'slot_labels' in sample:\n",
    "        slots = sample['slot_labels']\n",
    "    else:\n",
    "        slots = ['O'] * len(tokens)\n",
    "\n",
    "    return tokens, intent, slots\n",
    "\n",
    "def parse_slurp_sample(sample):\n",
    "    \"\"\"Parse SLURP JSON sample - handles multiple formats\"\"\"\n",
    "    # Try to get tokens from various possible fields\n",
    "    tokens = []\n",
    "    if 'tokens' in sample:\n",
    "        tokens = sample['tokens']\n",
    "    elif 'sentence' in sample:\n",
    "        # If sentence is a string, split it\n",
    "        sentence = sample['sentence']\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = sentence.split()\n",
    "        else:\n",
    "            tokens = sentence\n",
    "    elif 'text' in sample:\n",
    "        text = sample['text']\n",
    "        if isinstance(text, str):\n",
    "            tokens = text.split()\n",
    "        else:\n",
    "            tokens = text\n",
    "    \n",
    "    # Try to get intent from various possible fields\n",
    "    scenario = sample.get('scenario', '')\n",
    "    action = sample.get('action', '')\n",
    "    \n",
    "    # Check if intent is directly provided\n",
    "    if 'intent' in sample:\n",
    "        intent = sample['intent']\n",
    "    elif scenario and action:\n",
    "        intent = f\"{scenario}_{action}\"\n",
    "    elif scenario:\n",
    "        intent = scenario\n",
    "    elif action:\n",
    "        intent = action\n",
    "    else:\n",
    "        intent = ''\n",
    "    \n",
    "    # Get entities/slots\n",
    "    entities = sample.get('entities', [])\n",
    "    slots = ['O'] * len(tokens)\n",
    "\n",
    "    for entity in entities:\n",
    "        slot_type = entity.get('type', 'entity')\n",
    "        start = entity.get('start', 0)\n",
    "        end = entity.get('end', 0)\n",
    "\n",
    "        if start < len(slots):\n",
    "            slots[start] = f'B-{slot_type}'\n",
    "        for i in range(start + 1, min(end, len(slots))):\n",
    "            slots[i] = f'I-{slot_type}'\n",
    "\n",
    "    return tokens, intent, slots\n",
    "\n",
    "def load_json_dataset(file_path, dataset_type='atis'):\n",
    "    \"\"\"Load dataset from JSON Lines (.jsonl) file\"\"\"\n",
    "    print(f\"Loading {file_path}...\")\n",
    "\n",
    "    parsed_data = []\n",
    "    parse_fn = parse_atis_sample if dataset_type == 'atis' else parse_slurp_sample\n",
    "    error_count = 0\n",
    "    sample_shown = False\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line:  # Skip empty lines\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    sample = json.loads(line)\n",
    "                    \n",
    "                    # Show first sample for debugging\n",
    "                    if not sample_shown and line_num == 1:\n",
    "                        print(f\"\\n  First sample structure: {list(sample.keys())}\")\n",
    "                        print(f\"  Sample preview: {str(sample)[:200]}...\")\n",
    "                        sample_shown = True\n",
    "                    \n",
    "                    tokens, intent, slots = parse_fn(sample)\n",
    "                    \n",
    "                    # Validate the sample\n",
    "                    if tokens and intent:\n",
    "                        # Ensure slots match tokens length\n",
    "                        if len(slots) != len(tokens):\n",
    "                            slots = slots[:len(tokens)] + ['O'] * max(0, len(tokens) - len(slots))\n",
    "                        parsed_data.append((tokens, intent, slots))\n",
    "                    else:\n",
    "                        error_count += 1\n",
    "                        if error_count <= 3:  # Show first 3 errors with details\n",
    "                            print(f\"  Warning line {line_num}: Invalid sample\")\n",
    "                            print(f\"    - Has tokens: {bool(tokens)} (length: {len(tokens)})\")\n",
    "                            print(f\"    - Has intent: {bool(intent)} (value: '{intent}')\")\n",
    "                            \n",
    "                except json.JSONDecodeError as e:\n",
    "                    error_count += 1\n",
    "                    if error_count <= 3:\n",
    "                        print(f\"  Error line {line_num}: JSON decode error - {e}\")\n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    if error_count <= 3:\n",
    "                        print(f\"  Error line {line_num}: {e}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found - {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading file: {e}\")\n",
    "        return []\n",
    "\n",
    "    if error_count > 3:\n",
    "        print(f\"  ... and {error_count - 3} more errors\")\n",
    "    \n",
    "    print(f\"Loaded {len(parsed_data)} valid samples\")\n",
    "    \n",
    "    if len(parsed_data) == 0:\n",
    "        print(f\"\\n⚠️  WARNING: No valid samples loaded from {file_path}!\")\n",
    "        print(\"\\nPlease check the file format. The parser is looking for:\")\n",
    "        if dataset_type == 'slurp':\n",
    "            print(\"  - 'tokens' OR 'sentence' OR 'text': the input words\")\n",
    "            print(\"  - 'scenario' + 'action' OR 'intent': the intent label\")\n",
    "            print(\"  - 'entities': entity annotations (optional)\")\n",
    "        else:\n",
    "            print(\"  - 'tokens' or 'text': words/sentence\")\n",
    "            print(\"  - 'intent': intent label\")\n",
    "            print(\"  - 'slots': slot labels\")\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "def build_vocab_and_mappings(train_data, val_data, min_freq=1):\n",
    "    \"\"\"Build vocabulary and label mappings from data\"\"\"\n",
    "    print(\"\\nBuilding vocabulary and mappings...\")\n",
    "\n",
    "    if len(train_data) == 0 and len(val_data) == 0:\n",
    "        print(\"ERROR: No training or validation data available!\")\n",
    "        return {'<PAD>': 0, '<UNK>': 1}, {}, {}\n",
    "\n",
    "    # Collect all tokens, intents, and slots\n",
    "    all_tokens = []\n",
    "    all_intents = []\n",
    "    all_slots = []\n",
    "\n",
    "    for tokens, intent, slots in train_data + val_data:\n",
    "        all_tokens.extend([t.lower() for t in tokens])\n",
    "        all_intents.append(intent)\n",
    "        all_slots.extend(slots)\n",
    "\n",
    "    # Build vocabulary\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    idx = 2\n",
    "    for token, freq in token_counter.most_common():\n",
    "        if freq >= min_freq:\n",
    "            vocab[token] = idx\n",
    "            idx += 1\n",
    "\n",
    "    # Build intent mapping\n",
    "    unique_intents = sorted(set(all_intents))\n",
    "    intent2idx = {intent: idx for idx, intent in enumerate(unique_intents)}\n",
    "\n",
    "    # Build slot mapping\n",
    "    unique_slots = sorted(set(all_slots))\n",
    "    slot2idx = {slot: idx for idx, slot in enumerate(unique_slots)}\n",
    "\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    print(f\"Number of intents: {len(intent2idx)}\")\n",
    "    print(f\"Number of slots: {len(slot2idx)}\")\n",
    "\n",
    "    return vocab, intent2idx, slot2idx\n",
    "\n",
    "def convert_to_ids(data, vocab, intent2idx, slot2idx):\n",
    "    \"\"\"Convert tokens and labels to IDs\"\"\"\n",
    "    processed = []\n",
    "\n",
    "    for tokens, intent, slots in data:\n",
    "        # Convert tokens to IDs\n",
    "        input_ids = [vocab.get(t.lower(), vocab['<UNK>']) for t in tokens]\n",
    "\n",
    "        # Convert intent to ID\n",
    "        intent_id = intent2idx.get(intent, 0)\n",
    "\n",
    "        # Convert slots to IDs\n",
    "        slot_ids = [slot2idx.get(s, 0) for s in slots]\n",
    "\n",
    "        processed.append({\n",
    "            'input_ids': input_ids,\n",
    "            'intent': intent_id,\n",
    "            'slots': slot_ids\n",
    "        })\n",
    "\n",
    "    return processed\n",
    "\n",
    "\n",
    "\n",
    "def load_atis_csv(file_path):\n",
    "    \"\"\"Load ATIS data from CSV file - handles flexible column names\"\"\"\n",
    "    print(f\"Loading ATIS CSV from {file_path}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found - {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"CSV columns found: {list(df.columns)}\")\n",
    "    \n",
    "    parsed_data = []\n",
    "    \n",
    "    # Find tokens column (could be 'tokens', 'text', 'sentence', etc.)\n",
    "    tokens_col = None\n",
    "    for col in ['tokens', 'text', 'sentence', 'words']:\n",
    "        if col in df.columns:\n",
    "            tokens_col = col\n",
    "            break\n",
    "    \n",
    "    if tokens_col is None:\n",
    "        print(f\"ERROR: No tokens column found. Available columns: {list(df.columns)}\")\n",
    "        print(\"Expected one of: 'tokens', 'text', 'sentence', or 'words'\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Using '{tokens_col}' column for tokens\")\n",
    "    \n",
    "    # Find intent column\n",
    "    if 'intent' not in df.columns:\n",
    "        print(f\"ERROR: 'intent' column not found. Available columns: {list(df.columns)}\")\n",
    "        return []\n",
    "    \n",
    "    # Slots are optional\n",
    "    has_slots = 'slots' in df.columns\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        tokens_str = str(row[tokens_col]).strip()\n",
    "        \n",
    "        # Parse tokens - handle different formats\n",
    "        if tokens_str.startswith('['):\n",
    "            # JSON-like format: \"[token1, token2, ...]\"\n",
    "            try:\n",
    "                tokens = eval(tokens_str)\n",
    "            except:\n",
    "                tokens = tokens_str.split()\n",
    "        else:\n",
    "            # Space-separated: \"token1 token2 ...\"\n",
    "            tokens = tokens_str.split()\n",
    "        \n",
    "        intent = str(row['intent']).strip()\n",
    "        \n",
    "        if has_slots:\n",
    "            slots_str = str(row['slots']).strip()\n",
    "            if slots_str.startswith('['):\n",
    "                try:\n",
    "                    slots = eval(slots_str)\n",
    "                except:\n",
    "                    slots = slots_str.split()\n",
    "            else:\n",
    "                slots = slots_str.split()\n",
    "        else:\n",
    "            slots = ['O'] * len(tokens)\n",
    "        \n",
    "        if tokens and intent:\n",
    "            # Ensure slots match token length\n",
    "            if len(slots) != len(tokens):\n",
    "                slots = slots[:len(tokens)] + ['O'] * max(0, len(tokens) - len(slots))\n",
    "            \n",
    "            parsed_data.append((tokens, intent, slots))\n",
    "    \n",
    "    print(f\"Loaded {len(parsed_data)} samples from CSV\\n\")\n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "def create_train_val_split(train_data, val_split=0.2, random_state=42):\n",
    "    \"\"\"Split training data into train and validation sets\"\"\"\n",
    "    train_split, val_split_data = train_test_split(\n",
    "        train_data, \n",
    "        test_size=val_split, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    return train_split, val_split_data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ACTIVATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# ============================================================================\n",
    "# RNN CELL FROM SCRATCH\n",
    "# ============================================================================\n",
    "\n",
    "class RNNCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        limit_ih = np.sqrt(6 / (input_size + hidden_size))\n",
    "        limit_hh = np.sqrt(6 / (hidden_size + hidden_size))\n",
    "\n",
    "        self.W_ih = np.random.uniform(-limit_ih, limit_ih, (input_size, hidden_size))\n",
    "        self.W_hh = np.random.uniform(-limit_hh, limit_hh, (hidden_size, hidden_size))\n",
    "        self.b_h = np.zeros((1, hidden_size))\n",
    "\n",
    "        self.m_W_ih = np.zeros_like(self.W_ih)\n",
    "        self.v_W_ih = np.zeros_like(self.W_ih)\n",
    "        self.m_W_hh = np.zeros_like(self.W_hh)\n",
    "        self.v_W_hh = np.zeros_like(self.W_hh)\n",
    "        self.m_b_h = np.zeros_like(self.b_h)\n",
    "        self.v_b_h = np.zeros_like(self.b_h)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        self.x = x\n",
    "        self.h_prev = h_prev\n",
    "        self.h = tanh(x @ self.W_ih + h_prev @ self.W_hh + self.b_h)\n",
    "        return self.h\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        dh_raw = dh_next * tanh_derivative(self.h)\n",
    "\n",
    "        self.dW_ih = self.x.T @ dh_raw\n",
    "        self.dW_hh = self.h_prev.T @ dh_raw\n",
    "        self.db_h = np.sum(dh_raw, axis=0, keepdims=True)\n",
    "\n",
    "        dx = dh_raw @ self.W_ih.T\n",
    "        dh_prev = dh_raw @ self.W_hh.T\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "# ============================================================================\n",
    "# BIDIRECTIONAL RNN LAYER\n",
    "# ============================================================================\n",
    "\n",
    "class BiRNN:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forward_cells = []\n",
    "        self.backward_cells = []\n",
    "        self.max_cells = 0\n",
    "\n",
    "    def forward(self, X, seq_lengths=None):\n",
    "        batch_size, max_seq_len, _ = X.shape\n",
    "\n",
    "        # Dynamically create cells if needed\n",
    "        while len(self.forward_cells) < max_seq_len:\n",
    "            self.forward_cells.append(RNNCell(self.input_size, self.hidden_size))\n",
    "            self.backward_cells.append(RNNCell(self.input_size, self.hidden_size))\n",
    "        \n",
    "        self.max_cells = max(self.max_cells, max_seq_len)\n",
    "\n",
    "        h_forward = np.zeros((batch_size, self.hidden_size))\n",
    "        forward_hiddens = []\n",
    "\n",
    "        for t in range(max_seq_len):\n",
    "            h_forward = self.forward_cells[t].forward(X[:, t, :], h_forward)\n",
    "            forward_hiddens.append(h_forward)\n",
    "\n",
    "        h_backward = np.zeros((batch_size, self.hidden_size))\n",
    "        backward_hiddens = []\n",
    "\n",
    "        for t in range(max_seq_len - 1, -1, -1):\n",
    "            h_backward = self.backward_cells[t].forward(X[:, t, :], h_backward)\n",
    "            backward_hiddens.insert(0, h_backward)\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(max_seq_len):\n",
    "            outputs.append(np.concatenate([forward_hiddens[t], backward_hiddens[t]], axis=1))\n",
    "\n",
    "        outputs = np.stack(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, doutputs):\n",
    "        batch_size, max_seq_len, _ = doutputs.shape\n",
    "\n",
    "        dforward = doutputs[:, :, :self.hidden_size]\n",
    "        dbackward = doutputs[:, :, self.hidden_size:]\n",
    "\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
    "        for t in range(max_seq_len - 1, -1, -1):\n",
    "            dh = dforward[:, t, :] + dh_next\n",
    "            _, dh_next = self.forward_cells[t].backward(dh)\n",
    "\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
    "        for t in range(max_seq_len):\n",
    "            dh = dbackward[:, t, :] + dh_next\n",
    "            _, dh_next = self.backward_cells[t].backward(dh)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# LSTM CELL FROM SCRATCH\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# LSTM CELL FROM SCRATCH\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# LSTM CELL FROM SCRATCH\n",
    "# ============================================================================\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Xavier initialization\n",
    "        limit_ih = np.sqrt(6 / (input_size + hidden_size))\n",
    "        limit_hh = np.sqrt(6 / (hidden_size + hidden_size))\n",
    "\n",
    "        # Forget gate parameters\n",
    "        self.W_if = np.random.uniform(-limit_ih, limit_ih, (input_size, hidden_size))\n",
    "        self.W_hf = np.random.uniform(-limit_hh, limit_hh, (hidden_size, hidden_size))\n",
    "        self.b_f = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Input gate parameters\n",
    "        self.W_ii = np.random.uniform(-limit_ih, limit_ih, (input_size, hidden_size))\n",
    "        self.W_hi = np.random.uniform(-limit_hh, limit_hh, (hidden_size, hidden_size))\n",
    "        self.b_i = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Cell gate parameters\n",
    "        self.W_ig = np.random.uniform(-limit_ih, limit_ih, (input_size, hidden_size))\n",
    "        self.W_hg = np.random.uniform(-limit_hh, limit_hh, (hidden_size, hidden_size))\n",
    "        self.b_g = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Output gate parameters\n",
    "        self.W_io = np.random.uniform(-limit_ih, limit_ih, (input_size, hidden_size))\n",
    "        self.W_ho = np.random.uniform(-limit_hh, limit_hh, (hidden_size, hidden_size))\n",
    "        self.b_o = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Initialize Adam optimizer parameters for all weights\n",
    "        self.m_W_if = np.zeros_like(self.W_if)\n",
    "        self.v_W_if = np.zeros_like(self.W_if)\n",
    "        self.m_W_hf = np.zeros_like(self.W_hf)\n",
    "        self.v_W_hf = np.zeros_like(self.W_hf)\n",
    "        self.m_b_f = np.zeros_like(self.b_f)\n",
    "        self.v_b_f = np.zeros_like(self.b_f)\n",
    "\n",
    "        self.m_W_ii = np.zeros_like(self.W_ii)\n",
    "        self.v_W_ii = np.zeros_like(self.W_ii)\n",
    "        self.m_W_hi = np.zeros_like(self.W_hi)\n",
    "        self.v_W_hi = np.zeros_like(self.W_hi)\n",
    "        self.m_b_i = np.zeros_like(self.b_i)\n",
    "        self.v_b_i = np.zeros_like(self.b_i)\n",
    "\n",
    "        self.m_W_ig = np.zeros_like(self.W_ig)\n",
    "        self.v_W_ig = np.zeros_like(self.W_ig)\n",
    "        self.m_W_hg = np.zeros_like(self.W_hg)\n",
    "        self.v_W_hg = np.zeros_like(self.W_hg)\n",
    "        self.m_b_g = np.zeros_like(self.b_g)\n",
    "        self.v_b_g = np.zeros_like(self.b_g)\n",
    "\n",
    "        self.m_W_io = np.zeros_like(self.W_io)\n",
    "        self.v_W_io = np.zeros_like(self.W_io)\n",
    "        self.m_W_ho = np.zeros_like(self.W_ho)\n",
    "        self.v_W_ho = np.zeros_like(self.W_ho)\n",
    "        self.m_b_o = np.zeros_like(self.b_o)\n",
    "        self.v_b_o = np.zeros_like(self.b_o)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        self.x = x\n",
    "        self.h_prev = h_prev\n",
    "        self.c_prev = c_prev\n",
    "\n",
    "        # Forget gate\n",
    "        self.f_gate = self.sigmoid(x @ self.W_if + h_prev @ self.W_hf + self.b_f)\n",
    "        \n",
    "        # Input gate\n",
    "        self.i_gate = self.sigmoid(x @ self.W_ii + h_prev @ self.W_hi + self.b_i)\n",
    "        \n",
    "        # Cell gate (candidate)\n",
    "        self.g_gate = self.tanh(x @ self.W_ig + h_prev @ self.W_hg + self.b_g)\n",
    "        \n",
    "        # Output gate\n",
    "        self.o_gate = self.sigmoid(x @ self.W_io + h_prev @ self.W_ho + self.b_o)\n",
    "\n",
    "        # New cell state\n",
    "        self.c = self.f_gate * c_prev + self.i_gate * self.g_gate\n",
    "        \n",
    "        # New hidden state\n",
    "        self.h = self.o_gate * self.tanh(self.c)\n",
    "\n",
    "        return self.h, self.c\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        # Gradient through output gate\n",
    "        dtanh_c = dh_next * self.o_gate\n",
    "        dc = dc_next + dtanh_c * (1 - self.tanh(self.c) ** 2)\n",
    "\n",
    "        # Gradient through output gate\n",
    "        do_raw = dh_next * self.tanh(self.c)\n",
    "        do = do_raw * self.o_gate * (1 - self.o_gate)\n",
    "\n",
    "        # Gradient through cell state\n",
    "        df_raw = dc * self.c_prev\n",
    "        df = df_raw * self.f_gate * (1 - self.f_gate)\n",
    "\n",
    "        di_raw = dc * self.g_gate\n",
    "        di = di_raw * self.i_gate * (1 - self.i_gate)\n",
    "\n",
    "        dg_raw = dc * self.i_gate\n",
    "        dg = dg_raw * (1 - self.g_gate ** 2)\n",
    "\n",
    "        # Compute weight gradients\n",
    "        self.dW_if = self.x.T @ df\n",
    "        self.dW_hf = self.h_prev.T @ df\n",
    "        self.db_f = np.sum(df, axis=0, keepdims=True)\n",
    "\n",
    "        self.dW_ii = self.x.T @ di\n",
    "        self.dW_hi = self.h_prev.T @ di\n",
    "        self.db_i = np.sum(di, axis=0, keepdims=True)\n",
    "\n",
    "        self.dW_ig = self.x.T @ dg\n",
    "        self.dW_hg = self.h_prev.T @ dg\n",
    "        self.db_g = np.sum(dg, axis=0, keepdims=True)\n",
    "\n",
    "        self.dW_io = self.x.T @ do\n",
    "        self.dW_ho = self.h_prev.T @ do\n",
    "        self.db_o = np.sum(do, axis=0, keepdims=True)\n",
    "\n",
    "        # Compute gradients for inputs\n",
    "        dx = (df @ self.W_if.T + di @ self.W_ii.T + \n",
    "              dg @ self.W_ig.T + do @ self.W_io.T)\n",
    "        \n",
    "        dh_prev = (df @ self.W_hf.T + di @ self.W_hi.T + \n",
    "                   dg @ self.W_hg.T + do @ self.W_ho.T)\n",
    "        \n",
    "        dc_prev = dc * self.f_gate\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BIDIRECTIONAL LSTM LAYER\n",
    "# ============================================================================\n",
    "\n",
    "class BiLSTM:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forward_cells = []\n",
    "        self.backward_cells = []\n",
    "        self.max_cells = 0\n",
    "\n",
    "    def forward(self, X, seq_lengths=None):\n",
    "        batch_size, max_seq_len, _ = X.shape\n",
    "\n",
    "        # Dynamically create cells if needed\n",
    "        while len(self.forward_cells) < max_seq_len:\n",
    "            self.forward_cells.append(LSTMCell(self.input_size, self.hidden_size))\n",
    "            self.backward_cells.append(LSTMCell(self.input_size, self.hidden_size))\n",
    "        \n",
    "        self.max_cells = max(self.max_cells, max_seq_len)\n",
    "\n",
    "        # Forward pass\n",
    "        h_forward = np.zeros((batch_size, self.hidden_size))\n",
    "        c_forward = np.zeros((batch_size, self.hidden_size))\n",
    "        forward_hiddens = []\n",
    "\n",
    "        for t in range(max_seq_len):\n",
    "            h_forward, c_forward = self.forward_cells[t].forward(\n",
    "                X[:, t, :], h_forward, c_forward\n",
    "            )\n",
    "            forward_hiddens.append(h_forward)\n",
    "\n",
    "        # Backward pass\n",
    "        h_backward = np.zeros((batch_size, self.hidden_size))\n",
    "        c_backward = np.zeros((batch_size, self.hidden_size))\n",
    "        backward_hiddens = []\n",
    "\n",
    "        for t in range(max_seq_len - 1, -1, -1):\n",
    "            h_backward, c_backward = self.backward_cells[t].forward(\n",
    "                X[:, t, :], h_backward, c_backward\n",
    "            )\n",
    "            backward_hiddens.insert(0, h_backward)\n",
    "\n",
    "        # Concatenate forward and backward outputs\n",
    "        outputs = []\n",
    "        for t in range(max_seq_len):\n",
    "            outputs.append(np.concatenate([forward_hiddens[t], backward_hiddens[t]], axis=1))\n",
    "\n",
    "        outputs = np.stack(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, doutputs):\n",
    "        batch_size, max_seq_len, _ = doutputs.shape\n",
    "\n",
    "        dforward = doutputs[:, :, :self.hidden_size]\n",
    "        dbackward = doutputs[:, :, self.hidden_size:]\n",
    "\n",
    "        # Backward pass through forward direction\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
    "        dc_next = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        for t in range(max_seq_len - 1, -1, -1):\n",
    "            dh = dforward[:, t, :] + dh_next\n",
    "            _, dh_next, dc_next = self.forward_cells[t].backward(dh, dc_next)\n",
    "\n",
    "        # Backward pass through backward direction\n",
    "        dh_next = np.zeros((batch_size, self.hidden_size))\n",
    "        dc_next = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        for t in range(max_seq_len):\n",
    "            dh = dbackward[:, t, :] + dh_next\n",
    "            _, dh_next, dc_next = self.backward_cells[t].backward(dh, dc_next)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTENT CLASSIFIER WITH LSTM\n",
    "# ============================================================================\n",
    "\n",
    "class IntentClassifierLSTM:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_intents):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_intents = num_intents\n",
    "\n",
    "        self.embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "        self.encoder = BiLSTM(embed_dim, hidden_dim)\n",
    "\n",
    "        self.W_out = np.random.randn(2 * hidden_dim, num_intents) * 0.01\n",
    "        self.b_out = np.zeros((1, num_intents))\n",
    "\n",
    "        self.m_W_out = np.zeros_like(self.W_out)\n",
    "        self.v_W_out = np.zeros_like(self.W_out)\n",
    "        self.m_b_out = np.zeros_like(self.b_out)\n",
    "        self.v_b_out = np.zeros_like(self.b_out)\n",
    "        self.m_emb = np.zeros_like(self.embeddings)\n",
    "        self.v_emb = np.zeros_like(self.embeddings)\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, input_ids, training=True):\n",
    "        self.input_ids = input_ids\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        self.embeds = self.embeddings[input_ids]\n",
    "\n",
    "        if training:\n",
    "            self.dropout_mask = (np.random.rand(*self.embeds.shape) > 0.3).astype(float) / 0.7\n",
    "            embeds_dropped = self.embeds * self.dropout_mask\n",
    "        else:\n",
    "            embeds_dropped = self.embeds\n",
    "\n",
    "        self.rnn_outputs = self.encoder.forward(embeds_dropped)\n",
    "        self.final_hidden = self.rnn_outputs[:, -1, :]\n",
    "        self.logits = self.final_hidden @ self.W_out + self.b_out\n",
    "\n",
    "        return self.logits\n",
    "\n",
    "    def backward(self, dloss):\n",
    "        batch_size = dloss.shape[0]\n",
    "\n",
    "        dW_out = self.final_hidden.T @ dloss\n",
    "        db_out = np.sum(dloss, axis=0, keepdims=True)\n",
    "\n",
    "        dfinal_hidden = dloss @ self.W_out.T\n",
    "\n",
    "        drnn_outputs = np.zeros_like(self.rnn_outputs)\n",
    "        drnn_outputs[:, -1, :] = dfinal_hidden\n",
    "\n",
    "        self.encoder.backward(drnn_outputs)\n",
    "\n",
    "        return dW_out, db_out\n",
    "\n",
    "    def update(self, dW_out, db_out, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.t += 1\n",
    "\n",
    "        # Update output layer\n",
    "        self.m_W_out = beta1 * self.m_W_out + (1 - beta1) * dW_out\n",
    "        self.v_W_out = beta2 * self.v_W_out + (1 - beta2) * (dW_out ** 2)\n",
    "        m_hat = self.m_W_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_W_out / (1 - beta2 ** self.t)\n",
    "        self.W_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        self.m_b_out = beta1 * self.m_b_out + (1 - beta1) * db_out\n",
    "        self.v_b_out = beta2 * self.v_b_out + (1 - beta2) * (db_out ** 2)\n",
    "        m_hat = self.m_b_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_b_out / (1 - beta2 ** self.t)\n",
    "        self.b_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        # Update LSTM cells (same as IntentClassifierLSTM)\n",
    "        for cell in self.encoder.forward_cells + self.encoder.backward_cells:\n",
    "            # Update forget gate weights\n",
    "            cell.m_W_if = beta1 * cell.m_W_if + (1 - beta1) * cell.dW_if\n",
    "            cell.v_W_if = beta2 * cell.v_W_if + (1 - beta2) * (cell.dW_if ** 2)\n",
    "            m_hat_if = cell.m_W_if / (1 - beta1 ** self.t)\n",
    "            v_hat_if = cell.v_W_if / (1 - beta2 ** self.t)\n",
    "            cell.W_if -= lr * m_hat_if / (np.sqrt(v_hat_if) + eps)\n",
    "\n",
    "            cell.m_W_hf = beta1 * cell.m_W_hf + (1 - beta1) * cell.dW_hf\n",
    "            cell.v_W_hf = beta2 * cell.v_W_hf + (1 - beta2) * (cell.dW_hf ** 2)\n",
    "            m_hat_hf = cell.m_W_hf / (1 - beta1 ** self.t)\n",
    "            v_hat_hf = cell.v_W_hf / (1 - beta2 ** self.t)\n",
    "            cell.W_hf -= lr * m_hat_hf / (np.sqrt(v_hat_hf) + eps)\n",
    "\n",
    "            cell.m_b_f = beta1 * cell.m_b_f + (1 - beta1) * cell.db_f\n",
    "            cell.v_b_f = beta2 * cell.v_b_f + (1 - beta2) * (cell.db_f ** 2)\n",
    "            m_hat_bf = cell.m_b_f / (1 - beta1 ** self.t)\n",
    "            v_hat_bf = cell.v_b_f / (1 - beta2 ** self.t)\n",
    "            cell.b_f -= lr * m_hat_bf / (np.sqrt(v_hat_bf) + eps)\n",
    "\n",
    "            # Update input gate weights\n",
    "            cell.m_W_ii = beta1 * cell.m_W_ii + (1 - beta1) * cell.dW_ii\n",
    "            cell.v_W_ii = beta2 * cell.v_W_ii + (1 - beta2) * (cell.dW_ii ** 2)\n",
    "            m_hat_ii = cell.m_W_ii / (1 - beta1 ** self.t)\n",
    "            v_hat_ii = cell.v_W_ii / (1 - beta2 ** self.t)\n",
    "            cell.W_ii -= lr * m_hat_ii / (np.sqrt(v_hat_ii) + eps)\n",
    "\n",
    "            cell.m_W_hi = beta1 * cell.m_W_hi + (1 - beta1) * cell.dW_hi\n",
    "            cell.v_W_hi = beta2 * cell.v_W_hi + (1 - beta2) * (cell.dW_hi ** 2)\n",
    "            m_hat_hi = cell.m_W_hi / (1 - beta1 ** self.t)\n",
    "            v_hat_hi = cell.v_W_hi / (1 - beta2 ** self.t)\n",
    "            cell.W_hi -= lr * m_hat_hi / (np.sqrt(v_hat_hi) + eps)\n",
    "\n",
    "            cell.m_b_i = beta1 * cell.m_b_i + (1 - beta1) * cell.db_i\n",
    "            cell.v_b_i = beta2 * cell.v_b_i + (1 - beta2) * (cell.db_i ** 2)\n",
    "            m_hat_bi = cell.m_b_i / (1 - beta1 ** self.t)\n",
    "            v_hat_bi = cell.v_b_i / (1 - beta2 ** self.t)\n",
    "            cell.b_i -= lr * m_hat_bi / (np.sqrt(v_hat_bi) + eps)\n",
    "\n",
    "            # Update cell gate weights\n",
    "            cell.m_W_ig = beta1 * cell.m_W_ig + (1 - beta1) * cell.dW_ig\n",
    "            cell.v_W_ig = beta2 * cell.v_W_ig + (1 - beta2) * (cell.dW_ig ** 2)\n",
    "            m_hat_ig = cell.m_W_ig / (1 - beta1 ** self.t)\n",
    "            v_hat_ig = cell.v_W_ig / (1 - beta2 ** self.t)\n",
    "            cell.W_ig -= lr * m_hat_ig / (np.sqrt(v_hat_ig) + eps)\n",
    "\n",
    "            cell.m_W_hg = beta1 * cell.m_W_hg + (1 - beta1) * cell.dW_hg\n",
    "            cell.v_W_hg = beta2 * cell.v_W_hg + (1 - beta2) * (cell.dW_hg ** 2)\n",
    "            m_hat_hg = cell.m_W_hg / (1 - beta1 ** self.t)\n",
    "            v_hat_hg = cell.v_W_hg / (1 - beta2 ** self.t)\n",
    "            cell.W_hg -= lr * m_hat_hg / (np.sqrt(v_hat_hg) + eps)\n",
    "\n",
    "            cell.m_b_g = beta1 * cell.m_b_g + (1 - beta1) * cell.db_g\n",
    "            cell.v_b_g = beta2 * cell.v_b_g + (1 - beta2) * (cell.db_g ** 2)\n",
    "            m_hat_bg = cell.m_b_g / (1 - beta1 ** self.t)\n",
    "            v_hat_bg = cell.v_b_g / (1 - beta2 ** self.t)\n",
    "            cell.b_g -= lr * m_hat_bg / (np.sqrt(v_hat_bg) + eps)\n",
    "\n",
    "            # Update output gate weights\n",
    "            cell.m_W_io = beta1 * cell.m_W_io + (1 - beta1) * cell.dW_io\n",
    "            cell.v_W_io = beta2 * cell.v_W_io + (1 - beta2) * (cell.dW_io ** 2)\n",
    "            m_hat_io = cell.m_W_io / (1 - beta1 ** self.t)\n",
    "            v_hat_io = cell.v_W_io / (1 - beta2 ** self.t)\n",
    "            cell.W_io -= lr * m_hat_io / (np.sqrt(v_hat_io) + eps)\n",
    "\n",
    "            cell.m_W_ho = beta1 * cell.m_W_ho + (1 - beta1) * cell.dW_ho\n",
    "            cell.v_W_ho = beta2 * cell.v_W_ho + (1 - beta2) * (cell.dW_ho ** 2)\n",
    "            m_hat_ho = cell.m_W_ho / (1 - beta1 ** self.t)\n",
    "            v_hat_ho = cell.v_W_ho / (1 - beta2 ** self.t)\n",
    "            cell.W_ho -= lr * m_hat_ho / (np.sqrt(v_hat_ho) + eps)\n",
    "\n",
    "            cell.m_b_o = beta1 * cell.m_b_o + (1 - beta1) * cell.db_o\n",
    "            cell.v_b_o = beta2 * cell.v_b_o + (1 - beta2) * (cell.db_o ** 2)\n",
    "            m_hat_bo = cell.m_b_o / (1 - beta1 ** self.t)\n",
    "            v_hat_bo = cell.v_b_o / (1 - beta2 ** self.t)\n",
    "            cell.b_o -= lr * m_hat_bo / (np.sqrt(v_hat_bo) + eps)\n",
    "\n",
    "    def predict(self, input_ids, intent_ids):\n",
    "        logits = self.forward(input_ids, intent_ids, training=False)\n",
    "        return np.argmax(logits, axis=2)\n",
    "\n",
    "            \n",
    "\n",
    "    def predict(self, input_ids):\n",
    "        logits = self.forward(input_ids, training=False)\n",
    "        return np.argmax(logits, axis=1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SLOT FILLING MODEL WITH LSTM\n",
    "# ============================================================================\n",
    "\n",
    "class SlotFillingWithIntentLSTM:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_slots, num_intents):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_slots = num_slots\n",
    "        self.num_intents = num_intents\n",
    "\n",
    "        self.word_embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "        self.intent_embeddings = np.random.randn(num_intents, 32) * 0.01\n",
    "\n",
    "        self.encoder = BiLSTM(embed_dim + 32, hidden_dim)\n",
    "\n",
    "        self.W_out = np.random.randn(2 * hidden_dim, num_slots) * 0.01\n",
    "        self.b_out = np.zeros((1, num_slots))\n",
    "\n",
    "        self.m_W_out = np.zeros_like(self.W_out)\n",
    "        self.v_W_out = np.zeros_like(self.W_out)\n",
    "        self.m_b_out = np.zeros_like(self.b_out)\n",
    "        self.v_b_out = np.zeros_like(self.b_out)\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, input_ids, intent_ids, training=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.intent_ids = intent_ids\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        self.word_embeds = self.word_embeddings[input_ids]\n",
    "\n",
    "        intent_embeds = self.intent_embeddings[intent_ids]\n",
    "        self.intent_embeds = np.repeat(intent_embeds[:, np.newaxis, :], seq_len, axis=1)\n",
    "\n",
    "        self.combined = np.concatenate([self.word_embeds, self.intent_embeds], axis=2)\n",
    "\n",
    "        if training:\n",
    "            self.dropout_mask = (np.random.rand(*self.combined.shape) > 0.3).astype(float) / 0.7\n",
    "            combined_dropped = self.combined * self.dropout_mask\n",
    "        else:\n",
    "            combined_dropped = self.combined\n",
    "\n",
    "        self.rnn_outputs = self.encoder.forward(combined_dropped)\n",
    "\n",
    "        batch_size, seq_len, hidden_size = self.rnn_outputs.shape\n",
    "        rnn_flat = self.rnn_outputs.reshape(-1, hidden_size)\n",
    "        logits_flat = rnn_flat @ self.W_out + self.b_out\n",
    "        self.logits = logits_flat.reshape(batch_size, seq_len, self.num_slots)\n",
    "\n",
    "        return self.logits\n",
    "\n",
    "    def backward(self, dloss):\n",
    "        batch_size, seq_len, _ = dloss.shape\n",
    "        hidden_size = 2 * self.hidden_dim\n",
    "\n",
    "        dloss_flat = dloss.reshape(-1, self.num_slots)\n",
    "        rnn_flat = self.rnn_outputs.reshape(-1, hidden_size)\n",
    "\n",
    "        dW_out = rnn_flat.T @ dloss_flat\n",
    "        db_out = np.sum(dloss_flat, axis=0, keepdims=True)\n",
    "\n",
    "        drnn_flat = dloss_flat @ self.W_out.T\n",
    "        drnn_outputs = drnn_flat.reshape(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        self.encoder.backward(drnn_outputs)\n",
    "\n",
    "        return dW_out, db_out\n",
    "\n",
    "    def update(self, dW_out, db_out, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.t += 1\n",
    "\n",
    "        # Update output layer\n",
    "        self.m_W_out = beta1 * self.m_W_out + (1 - beta1) * dW_out\n",
    "        self.v_W_out = beta2 * self.v_W_out + (1 - beta2) * (dW_out ** 2)\n",
    "        m_hat = self.m_W_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_W_out / (1 - beta2 ** self.t)\n",
    "        self.W_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        self.m_b_out = beta1 * self.m_b_out + (1 - beta1) * db_out\n",
    "        self.v_b_out = beta2 * self.v_b_out + (1 - beta2) * (db_out ** 2)\n",
    "        m_hat = self.m_b_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_b_out / (1 - beta2 ** self.t)\n",
    "        self.b_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        # Update LSTM cells\n",
    "        for cell in self.encoder.forward_cells + self.encoder.backward_cells:\n",
    "            # Update forget gate weights\n",
    "            cell.m_W_if = beta1 * cell.m_W_if + (1 - beta1) * cell.dW_if\n",
    "            cell.v_W_if = beta2 * cell.v_W_if + (1 - beta2) * (cell.dW_if ** 2)\n",
    "            m_hat_if = cell.m_W_if / (1 - beta1 ** self.t)\n",
    "            v_hat_if = cell.v_W_if / (1 - beta2 ** self.t)\n",
    "            cell.W_if -= lr * m_hat_if / (np.sqrt(v_hat_if) + eps)\n",
    "\n",
    "            cell.m_W_hf = beta1 * cell.m_W_hf + (1 - beta1) * cell.dW_hf\n",
    "            cell.v_W_hf = beta2 * cell.v_W_hf + (1 - beta2) * (cell.dW_hf ** 2)\n",
    "            m_hat_hf = cell.m_W_hf / (1 - beta1 ** self.t)\n",
    "            v_hat_hf = cell.v_W_hf / (1 - beta2 ** self.t)\n",
    "            cell.W_hf -= lr * m_hat_hf / (np.sqrt(v_hat_hf) + eps)\n",
    "\n",
    "            cell.m_b_f = beta1 * cell.m_b_f + (1 - beta1) * cell.db_f\n",
    "            cell.v_b_f = beta2 * cell.v_b_f + (1 - beta2) * (cell.db_f ** 2)\n",
    "            m_hat_bf = cell.m_b_f / (1 - beta1 ** self.t)\n",
    "            v_hat_bf = cell.v_b_f / (1 - beta2 ** self.t)\n",
    "            cell.b_f -= lr * m_hat_bf / (np.sqrt(v_hat_bf) + eps)\n",
    "\n",
    "            # Update input gate weights\n",
    "            cell.m_W_ii = beta1 * cell.m_W_ii + (1 - beta1) * cell.dW_ii\n",
    "            cell.v_W_ii = beta2 * cell.v_W_ii + (1 - beta2) * (cell.dW_ii ** 2)\n",
    "            m_hat_ii = cell.m_W_ii / (1 - beta1 ** self.t)\n",
    "            v_hat_ii = cell.v_W_ii / (1 - beta2 ** self.t)\n",
    "            cell.W_ii -= lr * m_hat_ii / (np.sqrt(v_hat_ii) + eps)\n",
    "\n",
    "            cell.m_W_hi = beta1 * cell.m_W_hi + (1 - beta1) * cell.dW_hi\n",
    "            cell.v_W_hi = beta2 * cell.v_W_hi + (1 - beta2) * (cell.dW_hi ** 2)\n",
    "            m_hat_hi = cell.m_W_hi / (1 - beta1 ** self.t)\n",
    "            v_hat_hi = cell.v_W_hi / (1 - beta2 ** self.t)\n",
    "            cell.W_hi -= lr * m_hat_hi / (np.sqrt(v_hat_hi) + eps)\n",
    "\n",
    "            cell.m_b_i = beta1 * cell.m_b_i + (1 - beta1) * cell.db_i\n",
    "            cell.v_b_i = beta2 * cell.v_b_i + (1 - beta2) * (cell.db_i ** 2)\n",
    "            m_hat_bi = cell.m_b_i / (1 - beta1 ** self.t)\n",
    "            v_hat_bi = cell.v_b_i / (1 - beta2 ** self.t)\n",
    "            cell.b_i -= lr * m_hat_bi / (np.sqrt(v_hat_bi) + eps)\n",
    "\n",
    "            # Update cell gate weights\n",
    "            cell.m_W_ig = beta1 * cell.m_W_ig + (1 - beta1) * cell.dW_ig\n",
    "            cell.v_W_ig = beta2 * cell.v_W_ig + (1 - beta2) * (cell.dW_ig ** 2)\n",
    "            m_hat_ig = cell.m_W_ig / (1 - beta1 ** self.t)\n",
    "            v_hat_ig = cell.v_W_ig / (1 - beta2 ** self.t)\n",
    "            cell.W_ig -= lr * m_hat_ig / (np.sqrt(v_hat_ig) + eps)\n",
    "\n",
    "            cell.m_W_hg = beta1 * cell.m_W_hg + (1 - beta1) * cell.dW_hg\n",
    "            cell.v_W_hg = beta2 * cell.v_W_hg + (1 - beta2) * (cell.dW_hg ** 2)\n",
    "            m_hat_hg = cell.m_W_hg / (1 - beta1 ** self.t)\n",
    "            v_hat_hg = cell.v_W_hg / (1 - beta2 ** self.t)\n",
    "            cell.W_hg -= lr * m_hat_hg / (np.sqrt(v_hat_hg) + eps)\n",
    "\n",
    "            cell.m_b_g = beta1 * cell.m_b_g + (1 - beta1) * cell.db_g\n",
    "            cell.v_b_g = beta2 * cell.v_b_g + (1 - beta2) * (cell.db_g ** 2)\n",
    "            m_hat_bg = cell.m_b_g / (1 - beta1 ** self.t)\n",
    "            v_hat_bg = cell.v_b_g / (1 - beta2 ** self.t)\n",
    "            cell.b_g -= lr * m_hat_bg / (np.sqrt(v_hat_bg) + eps)\n",
    "\n",
    "            # Update output gate weights\n",
    "            cell.m_W_io = beta1 * cell.m_W_io + (1 - beta1) * cell.dW_io\n",
    "            cell.v_W_io = beta2 * cell.v_W_io + (1 - beta2) * (cell.dW_io ** 2)\n",
    "            m_hat_io = cell.m_W_io / (1 - beta1 ** self.t)\n",
    "            v_hat_io = cell.v_W_io / (1 - beta2 ** self.t)\n",
    "            cell.W_io -= lr * m_hat_io / (np.sqrt(v_hat_io) + eps)\n",
    "\n",
    "            cell.m_W_ho = beta1 * cell.m_W_ho + (1 - beta1) * cell.dW_ho\n",
    "            cell.v_W_ho = beta2 * cell.v_W_ho + (1 - beta2) * (cell.dW_ho ** 2)\n",
    "            m_hat_ho = cell.m_W_ho / (1 - beta1 ** self.t)\n",
    "            v_hat_ho = cell.v_W_ho / (1 - beta2 ** self.t)\n",
    "            cell.W_ho -= lr * m_hat_ho / (np.sqrt(v_hat_ho) + eps)\n",
    "\n",
    "            cell.m_b_o = beta1 * cell.m_b_o + (1 - beta1) * cell.db_o\n",
    "            cell.v_b_o = beta2 * cell.v_b_o + (1 - beta2) * (cell.db_o ** 2)\n",
    "            m_hat_bo = cell.m_b_o / (1 - beta1 ** self.t)\n",
    "            v_hat_bo = cell.v_b_o / (1 - beta2 ** self.t)\n",
    "            cell.b_o -= lr * m_hat_bo / (np.sqrt(v_hat_bo) + eps)\n",
    "\n",
    "    def predict(self, input_ids, intent_ids):\n",
    "        logits = self.forward(input_ids, intent_ids, training=False)\n",
    "        return np.argmax(logits, axis=2)\n",
    "# ============================================================================\n",
    "# INTENT CLASSIFIER MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class IntentClassifier:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_intents):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_intents = num_intents\n",
    "\n",
    "        self.embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "        self.encoder = BiRNN(embed_dim, hidden_dim)\n",
    "\n",
    "        self.W_out = np.random.randn(2 * hidden_dim, num_intents) * 0.01\n",
    "        self.b_out = np.zeros((1, num_intents))\n",
    "\n",
    "        self.m_W_out = np.zeros_like(self.W_out)\n",
    "        self.v_W_out = np.zeros_like(self.W_out)\n",
    "        self.m_b_out = np.zeros_like(self.b_out)\n",
    "        self.v_b_out = np.zeros_like(self.b_out)\n",
    "        self.m_emb = np.zeros_like(self.embeddings)\n",
    "        self.v_emb = np.zeros_like(self.embeddings)\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, input_ids, training=True):\n",
    "        self.input_ids = input_ids\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        self.embeds = self.embeddings[input_ids]\n",
    "\n",
    "        if training:\n",
    "            self.dropout_mask = (np.random.rand(*self.embeds.shape) > 0.3).astype(float) / 0.7\n",
    "            embeds_dropped = self.embeds * self.dropout_mask\n",
    "        else:\n",
    "            embeds_dropped = self.embeds\n",
    "\n",
    "        self.rnn_outputs = self.encoder.forward(embeds_dropped)\n",
    "        self.final_hidden = self.rnn_outputs[:, -1, :]\n",
    "        self.logits = self.final_hidden @ self.W_out + self.b_out\n",
    "\n",
    "        return self.logits\n",
    "\n",
    "    def backward(self, dloss):\n",
    "        batch_size = dloss.shape[0]\n",
    "\n",
    "        dW_out = self.final_hidden.T @ dloss\n",
    "        db_out = np.sum(dloss, axis=0, keepdims=True)\n",
    "\n",
    "        dfinal_hidden = dloss @ self.W_out.T\n",
    "\n",
    "        drnn_outputs = np.zeros_like(self.rnn_outputs)\n",
    "        drnn_outputs[:, -1, :] = dfinal_hidden\n",
    "\n",
    "        self.encoder.backward(drnn_outputs)\n",
    "\n",
    "        return dW_out, db_out\n",
    "\n",
    "    def update(self, dW_out, db_out, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.t += 1\n",
    "\n",
    "        self.m_W_out = beta1 * self.m_W_out + (1 - beta1) * dW_out\n",
    "        self.v_W_out = beta2 * self.v_W_out + (1 - beta2) * (dW_out ** 2)\n",
    "        m_hat = self.m_W_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_W_out / (1 - beta2 ** self.t)\n",
    "        self.W_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        self.m_b_out = beta1 * self.m_b_out + (1 - beta1) * db_out\n",
    "        self.v_b_out = beta2 * self.v_b_out + (1 - beta2) * (db_out ** 2)\n",
    "        m_hat = self.m_b_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_b_out / (1 - beta2 ** self.t)\n",
    "        self.b_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        for cell in self.encoder.forward_cells + self.encoder.backward_cells:\n",
    "            m_W_ih = beta1 * cell.m_W_ih + (1 - beta1) * cell.dW_ih\n",
    "            v_W_ih = beta2 * cell.v_W_ih + (1 - beta2) * (cell.dW_ih ** 2)\n",
    "            m_hat = m_W_ih / (1 - beta1 ** self.t)\n",
    "            v_hat = v_W_ih / (1 - beta2 ** self.t)\n",
    "            cell.W_ih -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_W_ih = m_W_ih\n",
    "            cell.v_W_ih = v_W_ih\n",
    "\n",
    "            m_W_hh = beta1 * cell.m_W_hh + (1 - beta1) * cell.dW_hh\n",
    "            v_W_hh = beta2 * cell.v_W_hh + (1 - beta2) * (cell.dW_hh ** 2)\n",
    "            m_hat = m_W_hh / (1 - beta1 ** self.t)\n",
    "            v_hat = v_W_hh / (1 - beta2 ** self.t)\n",
    "            cell.W_hh -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_W_hh = m_W_hh\n",
    "            cell.v_W_hh = v_W_hh\n",
    "\n",
    "            m_b_h = beta1 * cell.m_b_h + (1 - beta1) * cell.db_h\n",
    "            v_b_h = beta2 * cell.v_b_h + (1 - beta2) * (cell.db_h ** 2)\n",
    "            m_hat = m_b_h / (1 - beta1 ** self.t)\n",
    "            v_hat = v_b_h / (1 - beta2 ** self.t)\n",
    "            cell.b_h -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_b_h = m_b_h\n",
    "            cell.v_b_h = v_b_h\n",
    "\n",
    "    def predict(self, input_ids):\n",
    "        logits = self.forward(input_ids, training=False)\n",
    "        return np.argmax(logits, axis=1)\n",
    "\n",
    "# ============================================================================\n",
    "# SLOT FILLING MODEL WITH INTENT\n",
    "# ============================================================================\n",
    "\n",
    "class SlotFillingWithIntent:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_slots, num_intents):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_slots = num_slots\n",
    "        self.num_intents = num_intents\n",
    "\n",
    "        self.word_embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "        self.intent_embeddings = np.random.randn(num_intents, 32) * 0.01\n",
    "\n",
    "        self.encoder = BiRNN(embed_dim + 32, hidden_dim)\n",
    "\n",
    "        self.W_out = np.random.randn(2 * hidden_dim, num_slots) * 0.01\n",
    "        self.b_out = np.zeros((1, num_slots))\n",
    "\n",
    "        self.m_W_out = np.zeros_like(self.W_out)\n",
    "        self.v_W_out = np.zeros_like(self.W_out)\n",
    "        self.m_b_out = np.zeros_like(self.b_out)\n",
    "        self.v_b_out = np.zeros_like(self.b_out)\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, input_ids, intent_ids, training=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.intent_ids = intent_ids\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        self.word_embeds = self.word_embeddings[input_ids]\n",
    "\n",
    "        intent_embeds = self.intent_embeddings[intent_ids]\n",
    "        self.intent_embeds = np.repeat(intent_embeds[:, np.newaxis, :], seq_len, axis=1)\n",
    "\n",
    "        self.combined = np.concatenate([self.word_embeds, self.intent_embeds], axis=2)\n",
    "\n",
    "        if training:\n",
    "            self.dropout_mask = (np.random.rand(*self.combined.shape) > 0.3).astype(float) / 0.7\n",
    "            combined_dropped = self.combined * self.dropout_mask\n",
    "        else:\n",
    "            combined_dropped = self.combined\n",
    "\n",
    "        self.rnn_outputs = self.encoder.forward(combined_dropped)\n",
    "\n",
    "        batch_size, seq_len, hidden_size = self.rnn_outputs.shape\n",
    "        rnn_flat = self.rnn_outputs.reshape(-1, hidden_size)\n",
    "        logits_flat = rnn_flat @ self.W_out + self.b_out\n",
    "        self.logits = logits_flat.reshape(batch_size, seq_len, self.num_slots)\n",
    "\n",
    "        return self.logits\n",
    "\n",
    "    def backward(self, dloss):\n",
    "        batch_size, seq_len, _ = dloss.shape\n",
    "        hidden_size = 2 * self.hidden_dim\n",
    "\n",
    "        dloss_flat = dloss.reshape(-1, self.num_slots)\n",
    "        rnn_flat = self.rnn_outputs.reshape(-1, hidden_size)\n",
    "\n",
    "        dW_out = rnn_flat.T @ dloss_flat\n",
    "        db_out = np.sum(dloss_flat, axis=0, keepdims=True)\n",
    "\n",
    "        drnn_flat = dloss_flat @ self.W_out.T\n",
    "        drnn_outputs = drnn_flat.reshape(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        self.encoder.backward(drnn_outputs)\n",
    "\n",
    "        return dW_out, db_out\n",
    "\n",
    "    def update(self, dW_out, db_out, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.t += 1\n",
    "\n",
    "        self.m_W_out = beta1 * self.m_W_out + (1 - beta1) * dW_out\n",
    "        self.v_W_out = beta2 * self.v_W_out + (1 - beta2) * (dW_out ** 2)\n",
    "        m_hat = self.m_W_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_W_out / (1 - beta2 ** self.t)\n",
    "        self.W_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        self.m_b_out = beta1 * self.m_b_out + (1 - beta1) * db_out\n",
    "        self.v_b_out = beta2 * self.v_b_out + (1 - beta2) * (db_out ** 2)\n",
    "        m_hat = self.m_b_out / (1 - beta1 ** self.t)\n",
    "        v_hat = self.v_b_out / (1 - beta2 ** self.t)\n",
    "        self.b_out -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        for cell in self.encoder.forward_cells + self.encoder.backward_cells:\n",
    "            m_W_ih = beta1 * cell.m_W_ih + (1 - beta1) * cell.dW_ih\n",
    "            v_W_ih = beta2 * cell.v_W_ih + (1 - beta2) * (cell.dW_ih ** 2)\n",
    "            m_hat = m_W_ih / (1 - beta1 ** self.t)\n",
    "            v_hat = v_W_ih / (1 - beta2 ** self.t)\n",
    "            cell.W_ih -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_W_ih = m_W_ih\n",
    "            cell.v_W_ih = v_W_ih\n",
    "\n",
    "            m_W_hh = beta1 * cell.m_W_hh + (1 - beta1) * cell.dW_hh\n",
    "            v_W_hh = beta2 * cell.v_W_hh + (1 - beta2) * (cell.dW_hh ** 2)\n",
    "            m_hat = m_W_hh / (1 - beta1 ** self.t)\n",
    "            v_hat = v_W_hh / (1 - beta2 ** self.t)\n",
    "            cell.W_hh -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_W_hh = m_W_hh\n",
    "            cell.v_W_hh = v_W_hh\n",
    "\n",
    "            m_b_h = beta1 * cell.m_b_h + (1 - beta1) * cell.db_h\n",
    "            v_b_h = beta2 * cell.v_b_h + (1 - beta2) * (cell.db_h ** 2)\n",
    "            m_hat = m_b_h / (1 - beta1 ** self.t)\n",
    "            v_hat = v_b_h / (1 - beta2 ** self.t)\n",
    "            cell.b_h -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            cell.m_b_h = m_b_h\n",
    "            cell.v_b_h = v_b_h\n",
    "\n",
    "    def predict(self, input_ids, intent_ids):\n",
    "        logits = self.forward(input_ids, intent_ids, training=False)\n",
    "        return np.argmax(logits, axis=2)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def train_intent_classifier(model, train_data, val_data, epochs=15, batch_size=32, lr=0.001):\n",
    "    print(\"Training Intent Classifier...\")\n",
    "    \n",
    "    if len(train_data) == 0:\n",
    "        print(\"ERROR: No training data available!\")\n",
    "        return model\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(train_data)\n",
    "\n",
    "        total_loss = 0\n",
    "        num_batches = max(1, len(train_data) // batch_size)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Create progress bar\n",
    "        pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{epochs}\", ncols=100)\n",
    "        \n",
    "        for i in pbar:\n",
    "            batch = train_data[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "\n",
    "            max_len = max(len(item['input_ids']) for item in batch)\n",
    "            input_ids = np.zeros((len(batch), max_len), dtype=int)\n",
    "            intents = np.zeros(len(batch), dtype=int)\n",
    "\n",
    "            for j, item in enumerate(batch):\n",
    "                length = len(item['input_ids'])\n",
    "                input_ids[j, :length] = item['input_ids']\n",
    "                intents[j] = item['intent']\n",
    "\n",
    "            logits = model.forward(input_ids, training=True)\n",
    "\n",
    "            probs = softmax(logits)\n",
    "            loss = -np.mean(np.log(probs[range(len(batch)), intents] + 1e-10))\n",
    "            total_loss += loss\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            preds = np.argmax(logits, axis=1)\n",
    "            correct += np.sum(preds == intents)\n",
    "            total += len(batch)\n",
    "\n",
    "            dloss = probs.copy()\n",
    "            dloss[range(len(batch)), intents] -= 1\n",
    "            dloss /= len(batch)\n",
    "\n",
    "            dW_out, db_out = model.backward(dloss)\n",
    "            model.update(dW_out, db_out, lr=lr)\n",
    "            \n",
    "            # Update progress bar with current loss and accuracy\n",
    "            train_acc = correct / total if total > 0 else 0\n",
    "            pbar.set_postfix({'loss': f'{loss:.4f}', 'acc': f'{train_acc:.4f}'})\n",
    "\n",
    "        train_acc = correct / total if total > 0 else 0\n",
    "        val_acc = evaluate_intent(model, val_data, batch_size) if len(val_data) > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/num_batches:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(f\"  ✓ New best validation accuracy: {val_acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "def train_slot_filler(slot_model, intent_model, train_data, val_data, epochs=15, batch_size=32, lr=0.001):\n",
    "    print(\"Training Slot Filler...\")\n",
    "    \n",
    "    if len(train_data) == 0:\n",
    "        print(\"ERROR: No training data available!\")\n",
    "        return slot_model\n",
    "\n",
    "    best_f1 = 0\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(train_data)\n",
    "\n",
    "        total_loss = 0\n",
    "        num_batches = max(1, len(train_data) // batch_size)\n",
    "\n",
    "        # Create progress bar\n",
    "        pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{epochs}\", ncols=100)\n",
    "        \n",
    "        for i in pbar:\n",
    "            batch = train_data[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            if len(batch) == 0:\n",
    "                continue\n",
    "\n",
    "            max_len = max(len(item['input_ids']) for item in batch)\n",
    "            input_ids = np.zeros((len(batch), max_len), dtype=int)\n",
    "            slots = np.full((len(batch), max_len), -100, dtype=int)\n",
    "\n",
    "            for j, item in enumerate(batch):\n",
    "                length = len(item['input_ids'])\n",
    "                input_ids[j, :length] = item['input_ids']\n",
    "                slots[j, :length] = item['slots']\n",
    "\n",
    "            predicted_intents = intent_model.predict(input_ids)\n",
    "\n",
    "            logits = slot_model.forward(input_ids, predicted_intents, training=True)\n",
    "\n",
    "            probs = softmax(logits)\n",
    "            loss = 0\n",
    "            count = 0\n",
    "\n",
    "            for b in range(len(batch)):\n",
    "                for t in range(max_len):\n",
    "                    if slots[b, t] != -100:\n",
    "                        loss += -np.log(probs[b, t, slots[b, t]] + 1e-10)\n",
    "                        count += 1\n",
    "\n",
    "            if count > 0:\n",
    "                loss /= count\n",
    "                total_loss += loss\n",
    "\n",
    "                dloss = probs.copy()\n",
    "                for b in range(len(batch)):\n",
    "                    for t in range(max_len):\n",
    "                        if slots[b, t] != -100:\n",
    "                            dloss[b, t, slots[b, t]] -= 1\n",
    "                dloss /= count\n",
    "\n",
    "                dW_out, db_out = slot_model.backward(dloss)\n",
    "                slot_model.update(dW_out, db_out, lr=lr)\n",
    "                \n",
    "                # Update progress bar with current loss\n",
    "                pbar.set_postfix({'loss': f'{loss:.4f}'})\n",
    "\n",
    "        metrics = evaluate_slot(slot_model, intent_model, val_data, batch_size) if len(val_data) > 0 else {'f1': 0, 'accuracy': 0}\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/num_batches:.4f}, Val F1: {metrics['f1']:.4f}, Val Acc: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            print(f\"  ✓ New best validation F1: {metrics['f1']:.4f}\")\n",
    "    return slot_model\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_intent(model, data, batch_size=32):\n",
    "    \"\"\"Evaluate intent classifier\"\"\"\n",
    "    if len(data) == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    num_batches = max(1, len(data) // batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = data[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        if len(batch) == 0:\n",
    "            continue\n",
    "\n",
    "        max_len = max(len(item['input_ids']) for item in batch)\n",
    "        input_ids = np.zeros((len(batch), max_len), dtype=int)\n",
    "        intents = np.zeros(len(batch), dtype=int)\n",
    "\n",
    "        for j, item in enumerate(batch):\n",
    "            length = len(item['input_ids'])\n",
    "            input_ids[j, :length] = item['input_ids']\n",
    "            intents[j] = item['intent']\n",
    "\n",
    "        intent_preds = model.predict(input_ids)\n",
    "\n",
    "        all_preds.extend(intent_preds)\n",
    "        all_labels.extend(intents)\n",
    "\n",
    "    if len(all_preds) == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate_slot(slot_model, intent_model, data, batch_size=32):\n",
    "    \"\"\"Evaluate slot filling model\"\"\"\n",
    "    if len(data) == 0:\n",
    "        return {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "        \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    num_batches = max(1, len(data) // batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = data[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        if len(batch) == 0:\n",
    "            continue\n",
    "\n",
    "        max_len = max(len(item['input_ids']) for item in batch)\n",
    "        input_ids = np.zeros((len(batch), max_len), dtype=int)\n",
    "        slots = np.full((len(batch), max_len), -100, dtype=int)\n",
    "        lengths = []\n",
    "\n",
    "        for j, item in enumerate(batch):\n",
    "            length = len(item['input_ids'])\n",
    "            input_ids[j, :length] = item['input_ids']\n",
    "            slots[j, :length] = item['slots']\n",
    "            lengths.append(length)\n",
    "\n",
    "        predicted_intents = intent_model.predict(input_ids)\n",
    "        slot_preds = slot_model.predict(input_ids, predicted_intents)\n",
    "\n",
    "        for j in range(len(batch)):\n",
    "            all_preds.extend(slot_preds[j, :lengths[j]])\n",
    "            all_labels.extend(slots[j, :lengths[j]])\n",
    "\n",
    "    if len(all_preds) == 0:\n",
    "        return {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "\n",
    "def main(train_file=None, val_file=None, test_file=None, dataset_type='slurp', model_type='rnn'):\n",
    "    \"\"\"Main function to run the entire pipeline\"\"\"\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    # Check if running in notebook or command line\n",
    "    if train_file is None:\n",
    "        if len(sys.argv) < 4:\n",
    "            print(\"⚠️  No file paths provided. Please call main() with file paths:\")\n",
    "            print(\"Example for SLURP: main('train.jsonl', 'val.jsonl', 'test.jsonl', 'slurp')\")\n",
    "            print(\"Example for ATIS: main('train.csv', 'test.csv', dataset_type='atis')\")\n",
    "            return None\n",
    "        \n",
    "        train_file = sys.argv[1]\n",
    "        test_file = sys.argv[2]\n",
    "        val_file = sys.argv[3] if len(sys.argv) > 3 else None\n",
    "        dataset_type = sys.argv[4] if len(sys.argv) > 4 else 'slurp'\n",
    "        model_type = sys.argv[5] if len(sys.argv) > 5 else 'rnn'\n",
    "    \n",
    "    print(f\"Dataset type: {dataset_type}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load datasets based on type\n",
    "    if dataset_type == 'atis':\n",
    "        # For ATIS: train_file and val_file are actually train.csv and test.csv\n",
    "        print(\"Loading ATIS CSV datasets...\")\n",
    "        train_data = load_atis_csv(train_file)\n",
    "        test_data = load_atis_csv(val_file) if val_file else []\n",
    "        \n",
    "        if len(train_data) == 0:\n",
    "            print(\"\\nERROR: No training data loaded!\")\n",
    "            return None\n",
    "        \n",
    "        # Auto split train into train/val\n",
    "        print(\"\\nSplitting training data into train/val (80/20)...\")\n",
    "        train_data, val_data = create_train_val_split(train_data, val_split=0.2)\n",
    "        print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "        \n",
    "    else:  # slurp or jsonl format\n",
    "        # Validate files exist\n",
    "        for file_path in [train_file, val_file, test_file]:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Error: File not found - {file_path}\")\n",
    "                return None\n",
    "        \n",
    "        # Load datasets\n",
    "        train_data = load_json_dataset(train_file, dataset_type)\n",
    "        val_data = load_json_dataset(val_file, dataset_type)\n",
    "        test_data = load_json_dataset(test_file, dataset_type)\n",
    "    \n",
    "    # Check if we have any data\n",
    "    if len(train_data) == 0:\n",
    "        print(\"\\nCRITICAL ERROR: No training data loaded!\")\n",
    "        return None\n",
    "    \n",
    "    # Build vocabulary and mappings\n",
    "    vocab, intent2idx, slot2idx = build_vocab_and_mappings(train_data, val_data)\n",
    "    \n",
    "    if len(intent2idx) == 0:\n",
    "        print(\"\\nERROR: No intents found in data!\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to IDs\n",
    "    train_processed = convert_to_ids(train_data, vocab, intent2idx, slot2idx)\n",
    "    val_processed = convert_to_ids(val_data, vocab, intent2idx, slot2idx)\n",
    "    test_processed = convert_to_ids(test_data, vocab, intent2idx, slot2idx)\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    EMBED_DIM = 256\n",
    "    HIDDEN_DIM = 256\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 25\n",
    "    LR = 0.0005\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PHASE 1: Training Intent Classifier ({model_type.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize intent classifier based on model type\n",
    "    if model_type == 'lstm':\n",
    "        intent_model = IntentClassifierLSTM(\n",
    "            vocab_size=len(vocab),\n",
    "            embed_dim=EMBED_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_intents=len(intent2idx)\n",
    "        )\n",
    "    else:  # 'rnn'\n",
    "        intent_model = IntentClassifier(\n",
    "            vocab_size=len(vocab),\n",
    "            embed_dim=EMBED_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_intents=len(intent2idx)\n",
    "        )\n",
    "    \n",
    "    intent_model = train_intent_classifier(\n",
    "        intent_model, \n",
    "        train_processed, \n",
    "        val_processed,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR\n",
    "    )\n",
    "    \n",
    "    # Evaluate intent classifier on test set\n",
    "    print(\"\\nEvaluating Intent Classifier on Test Set...\")\n",
    "    test_intent_acc = evaluate_intent(intent_model, test_processed, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test Intent Accuracy: {test_intent_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PHASE 2: Training Slot Filler ({model_type.upper()})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize slot filler based on model type\n",
    "    if model_type == 'lstm':\n",
    "        slot_model = SlotFillingWithIntentLSTM(\n",
    "            vocab_size=len(vocab),\n",
    "            embed_dim=EMBED_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_slots=len(slot2idx),\n",
    "            num_intents=len(intent2idx)\n",
    "        )\n",
    "    else:  # 'rnn'\n",
    "        slot_model = SlotFillingWithIntent(\n",
    "            vocab_size=len(vocab),\n",
    "            embed_dim=EMBED_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_slots=len(slot2idx),\n",
    "            num_intents=len(intent2idx)\n",
    "        )\n",
    "    \n",
    "    slot_model = train_slot_filler(\n",
    "        slot_model,\n",
    "        intent_model,\n",
    "        train_processed,\n",
    "        val_processed,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR\n",
    "    )\n",
    "    \n",
    "    # Evaluate slot filler on test set\n",
    "    print(\"\\nEvaluating Slot Filler on Test Set...\")\n",
    "    test_metrics = evaluate_slot(slot_model, intent_model, test_processed, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test Slot Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Test Slot Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"Test Slot Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"Test Slot F1: {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Intent Classification - Test Accuracy: {test_intent_acc:.4f}\")\n",
    "    print(f\"Slot Filling - Test F1: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"Slot Filling - Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'intent_model': intent_model,\n",
    "        'slot_model': slot_model,\n",
    "        'vocab': vocab,\n",
    "        'intent2idx': intent2idx,\n",
    "        'slot2idx': slot2idx,\n",
    "        'test_intent_acc': test_intent_acc,\n",
    "        'test_slot_metrics': test_metrics\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_ipython()\n",
    "        IN_NOTEBOOK = True\n",
    "    except NameError:\n",
    "        IN_NOTEBOOK = False\n",
    "    \n",
    "    if IN_NOTEBOOK:\n",
    "        print(\"Running in Jupyter Notebook mode\")\n",
    "        \n",
    "        # For SLURP:\n",
    "        # results = main(\n",
    "        #     train_file='../cleaned-datasets/slurp/train.jsonl',\n",
    "        #     val_file='../cleaned-datasets/slurp/devel.jsonl',\n",
    "        #     test_file='../cleaned-datasets/slurp/test.jsonl',\n",
    "        #     dataset_type='slurp',\n",
    "        #     model_type='lstm'\n",
    "        # )\n",
    "        \n",
    "        # For ATIS (uncomment below):\n",
    "        results = main(\n",
    "            train_file='../cleaned-datasets/atis/train.csv',\n",
    "            val_file='../cleaned-datasets/atis/test.csv',\n",
    "            test_file=None,\n",
    "            dataset_type='atis',\n",
    "            model_type='lstm'\n",
    "        )\n",
    "    else:\n",
    "        print(\"Running in command-line mode\")\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
