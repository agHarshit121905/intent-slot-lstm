{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7646fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: nan\n",
      "=== Validation ===\n",
      "Intent Acc: 0.6763 | Slot P: 0.1178 R: 0.1178 F1: 0.1178\n",
      "Epoch 2 | Train Loss: 2.1144\n",
      "=== Validation ===\n",
      "Intent Acc: 0.7723 | Slot P: 0.1407 R: 0.1407 F1: 0.1407\n",
      "Epoch 3 | Train Loss: nan\n",
      "=== Validation ===\n",
      "Intent Acc: 0.8106 | Slot P: 0.1518 R: 0.1518 F1: 0.1518\n",
      "Epoch 4 | Train Loss: nan\n",
      "=== Validation ===\n",
      "Intent Acc: 0.8066 | Slot P: 0.1576 R: 0.1576 F1: 0.1576\n",
      "Epoch 5 | Train Loss: nan\n",
      "=== Validation ===\n",
      "Intent Acc: 0.8294 | Slot P: 0.1655 R: 0.1655 F1: 0.1655\n",
      "Epoch 6 | Train Loss: nan\n",
      "=== Validation ===\n",
      "Intent Acc: 0.8126 | Slot P: 0.1663 R: 0.1663 F1: 0.1663\n",
      "Epoch 7 | Train Loss: 0.4424\n",
      "=== Validation ===\n",
      "Intent Acc: 0.8173 | Slot P: 0.1691 R: 0.1691 F1: 0.1691\n",
      "Epoch 8 | Train Loss: 0.3277\n",
      "=== Validation ===\n",
      "Intent Acc: 0.8173 | Slot P: 0.1706 R: 0.1706 F1: 0.1706\n",
      "Epoch 9 | Train Loss: 0.2416\n",
      "=== Validation ===\n",
      "Intent Acc: 0.8247 | Slot P: 0.1736 R: 0.1736 F1: 0.1736\n",
      "Epoch 10 | Train Loss: nan\n",
      "=== Validation ===\n",
      "Intent Acc: 0.8234 | Slot P: 0.1739 R: 0.1739 F1: 0.1739\n",
      "=== Test Set Evaluation ===\n",
      "Intent Acc: 0.8175 | Slot P: 0.1682 R: 0.1682 F1: 0.1682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8174640037157455,\n",
       " 0.16817636872252564,\n",
       " 0.16817636872252564,\n",
       " 0.16817636872252564)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def jsonl_to_df(jsonl_file):\n",
    "    texts, slot_seqs, intents = [], [], []\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            sentence = data[\"sentence\"]\n",
    "            tokens = sentence.split()\n",
    "            slots = [\"O\"] * len(tokens)\n",
    "            for entity in data.get(\"entities\", []):\n",
    "                span = entity[\"span\"]\n",
    "                typ = entity[\"type\"]\n",
    "                for i, token_idx in enumerate(span):\n",
    "                    if token_idx < len(tokens):\n",
    "                        slots[token_idx] = f\"B-{typ}\" if i == 0 else f\"I-{typ}\"\n",
    "            texts.append(sentence)\n",
    "            slot_seqs.append(\" \".join(slots))\n",
    "            intents.append(data[\"intent\"])\n",
    "    return pd.DataFrame({\"text\": texts, \"slots\": slot_seqs, \"intent\": intents})\n",
    "\n",
    "\n",
    "train_df = jsonl_to_df(\"train.jsonl\")\n",
    "val_df   = jsonl_to_df(\"devel.jsonl\")\n",
    "test_df  = jsonl_to_df(\"test.jsonl\")\n",
    "\n",
    "all_tokens = []\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    for sentence in df['text']:\n",
    "        all_tokens.extend(sentence.split())\n",
    "vocab = sorted(set(all_tokens))\n",
    "word2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "idx = 2\n",
    "for w in vocab:\n",
    "    if w not in word2idx:  \n",
    "        word2idx[w] = idx\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "all_slots = set()\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    for slots in df['slots']:\n",
    "        all_slots.update(slots.split())\n",
    "slot2id = {\"O\": 0}\n",
    "for s in sorted(all_slots):\n",
    "    if s != \"O\":\n",
    "        slot2id[s] = len(slot2id)\n",
    "\n",
    "\n",
    "all_intents = set()\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    all_intents.update(df['intent'].unique())\n",
    "all_intents = sorted(all_intents)\n",
    "intent2id = {intent: i for i, intent in enumerate(all_intents)}\n",
    "\n",
    "# Reverse mappings\n",
    "id2slot = {v: k for k, v in slot2id.items()}\n",
    "id2intent = {v: k for k, v in intent2id.items()}\n",
    "\n",
    "class SLURPDataset(Dataset):\n",
    "    def __init__(self, df, word2idx, slot2id, intent2id, max_len=50):\n",
    "        self.df = df\n",
    "        self.word2idx = word2idx\n",
    "        self.slot2id = slot2id\n",
    "        self.intent2id = intent2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.df.iloc[idx]['text'].split()\n",
    "        slots = self.df.iloc[idx]['slots'].split()\n",
    "        intent = self.df.iloc[idx]['intent']\n",
    "        length = min(len(tokens), self.max_len)  # Clip to max_len\n",
    "\n",
    "        input_ids = [self.word2idx.get(t, self.word2idx[\"<unk>\"]) for t in tokens[:self.max_len]]\n",
    "        slot_ids = [self.slot2id.get(s, 0) for s in slots[:self.max_len]]\n",
    "        intent_id = self.intent2id[intent]\n",
    "\n",
    "        pad_len = max(0, self.max_len - len(input_ids))\n",
    "        input_ids += [self.word2idx[\"<pad>\"]] * pad_len\n",
    "        slot_ids += [0] * pad_len\n",
    "\n",
    "        return torch.tensor(input_ids), torch.tensor(length), torch.tensor(slot_ids), torch.tensor(intent_id)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    lengths = torch.stack([item[1] for item in batch])\n",
    "    slot_labels = torch.stack([item[2] for item in batch])\n",
    "    intent_labels = torch.stack([item[3] for item in batch])\n",
    "    return input_ids, lengths, slot_labels, intent_labels\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(SLURPDataset(train_df, word2idx, slot2id, intent2id),\n",
    "                          batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(SLURPDataset(val_df, word2idx, slot2id, intent2id),\n",
    "                        batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(SLURPDataset(test_df, word2idx, slot2id, intent2id),\n",
    "                         batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class JointLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, slot_label_size, intent_label_size, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.slot_classifier = nn.Linear(hidden_dim * 2, slot_label_size)\n",
    "        self.intent_classifier = nn.Linear(hidden_dim * 2, intent_label_size)\n",
    "\n",
    "    def forward(self, input_ids, lengths):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (hidden, cell) = self.encoder(packed)\n",
    "        sequence_output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True, total_length=input_ids.size(1))\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        intent_logits = self.intent_classifier(hidden_cat)\n",
    "        return slot_logits, intent_logits\n",
    "\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "slot_label_size = len(slot2id)\n",
    "intent_label_size = len(intent2id)\n",
    "\n",
    "model = JointLSTMModel(vocab_size, embedding_dim, hidden_dim, slot_label_size, intent_label_size).to(device)\n",
    "slot_loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "intent_loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_intent_preds, all_intent_labels = [], []\n",
    "    all_slot_preds, all_slot_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, lengths, slot_labels, intent_labels in loader:\n",
    "            input_ids, lengths = input_ids.to(device), lengths.to(device)\n",
    "            slot_labels, intent_labels = slot_labels.to(device), intent_labels.to(device)\n",
    "\n",
    "            slot_logits, intent_logits = model(input_ids, lengths)\n",
    "            intent_preds = torch.argmax(intent_logits, dim=1)\n",
    "            all_intent_preds.extend(intent_preds.cpu().tolist())\n",
    "            all_intent_labels.extend(intent_labels.cpu().tolist())\n",
    "\n",
    "            slot_preds = torch.argmax(slot_logits, dim=2)\n",
    "            for i, l in enumerate(lengths):\n",
    "                all_slot_preds.extend(slot_preds[i][:l].cpu().tolist())\n",
    "                all_slot_labels.extend(slot_labels[i][:l].cpu().tolist())\n",
    "\n",
    "    intent_acc = accuracy_score(all_intent_labels, all_intent_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_slot_labels, all_slot_preds, average='micro', zero_division=0)\n",
    "    print(f\"Intent Acc: {intent_acc:.4f} | Slot P: {precision:.4f} R: {recall:.4f} F1: {f1:.4f}\")\n",
    "    model.train()\n",
    "    return intent_acc, precision, recall, f1\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, epochs=10, lambda_intent=0.5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for input_ids, lengths, slot_labels, intent_labels in train_loader:\n",
    "            input_ids, lengths = input_ids.to(device), lengths.to(device)\n",
    "            slot_labels, intent_labels = slot_labels.to(device), intent_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            slot_logits, intent_logits = model(input_ids, lengths)\n",
    "            slot_loss = slot_loss_fn(slot_logits.view(-1, slot_logits.shape[-1]), slot_labels.view(-1))\n",
    "            intent_loss = intent_loss_fn(intent_logits, intent_labels)\n",
    "            loss = slot_loss + lambda_intent * intent_loss\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        print(\"=== Validation ===\")\n",
    "        evaluate(model, val_loader)\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer, epochs=10)\n",
    "\n",
    "print(\"=== Test Set Evaluation ===\")\n",
    "evaluate(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
